SignatureAI.tsx

import React, { useState, useRef } from 'react';
import Layout from '@/components/Layout';
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Label } from '@/components/ui/label';
import { Badge } from '@/components/ui/badge';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { FileUpload } from '@/components/ui/file-upload';
import { Separator } from '@/components/ui/separator';
import { Dialog, DialogContent, DialogHeader, DialogTitle } from '@/components/ui/dialog';
import { DropdownMenu, DropdownMenuContent, DropdownMenuItem, DropdownMenuTrigger } from '@/components/ui/dropdown-menu';
import { useToast } from '@/components/ui/use-toast';
import { UnsavedChangesDialog } from '@/components/UnsavedChangesDialog';
import { useUnsavedChanges } from '@/hooks/useUnsavedChanges';
import { useNavigate, useLocation } from 'react-router-dom';
import { 
  Upload, 
  Camera, 
  Brain, 
  Scan, 
  CheckCircle, 
  XCircle, 
  AlertCircle,
  Loader2,
  ChevronLeft,
  ChevronRight,
  ChevronDown,
  User,
  MoreVertical,
  Trash2,
  AlertTriangle
} from 'lucide-react';
import { aiService, AI_CONFIG } from '@/lib/aiService';
import { fetchStudents } from '@/lib/supabaseService';
import type { Student } from '@/types';

interface TrainingFile {
  file: File;
  preview: string;
}

const SignatureAI = () => {
  const { toast } = useToast();
  
  // Training Section State
  const [genuineFiles, setGenuineFiles] = useState<TrainingFile[]>([]);
  const [forgedFiles, setForgedFiles] = useState<TrainingFile[]>([]);
  const [currentTrainingSet, setCurrentTrainingSet] = useState<'genuine' | 'forged'>('genuine');
  const [studentId, setStudentId] = useState<string>('');
  const [isTraining, setIsTraining] = useState(false);
  const [trainingResult, setTrainingResult] = useState<any>(null);
  
  // Training Progress State
  const [trainingProgress, setTrainingProgress] = useState(0);
  const [trainingStatus, setTrainingStatus] = useState<string>('');
  const [trainingStage, setTrainingStage] = useState<'idle' | 'preprocessing' | 'training' | 'validation' | 'completed' | 'error'>('idle');
  const [estimatedTimeRemaining, setEstimatedTimeRemaining] = useState<string>('');
  
  // Verification Section State
  const [verificationFile, setVerificationFile] = useState<File | null>(null);
  const [verificationPreview, setVerificationPreview] = useState<string>('');
  const [isVerifying, setIsVerifying] = useState(false);
  const [verificationResult, setVerificationResult] = useState<any>(null);
  const [useCamera, setUseCamera] = useState(false);
  const [referenceFiles, setReferenceFiles] = useState<File[]>([]);
  
  // Modal State
  const [isModalOpen, setIsModalOpen] = useState(false);
  const [modalImageIndex, setModalImageIndex] = useState(0);
  const [modalImages, setModalImages] = useState<string[]>([]);
  
  // Dropdown State
  const [isDropdownOpen, setIsDropdownOpen] = useState(false);
  const [isConfirmRemoveOpen, setIsConfirmRemoveOpen] = useState(false);
  const [visibleCounts, setVisibleCounts] = useState<{ genuine: number; forged: number }>({ genuine: 60, forged: 60 });
  const STORAGE_KEY = 'signatureAIState:v1';
  const NAVIGATION_FLAG = 'signatureAIInternalNav:v1';

  // Unsaved changes handling
  const navigate = useNavigate();
  const location = useLocation();
  const [pendingNavigation, setPendingNavigation] = useState<string | null>(null);
  
  const {
    hasUnsavedChanges,
    showConfirmDialog,
    markAsChanged,
    markAsSaved,
    handleClose,
    confirmClose,
    cancelClose,
    handleOpenChange,
  } = useUnsavedChanges({
    onClose: () => {
      // Execute the pending navigation when user confirms
      if (pendingNavigation) {
        navigate(pendingNavigation);
        setPendingNavigation(null);
      }
    },
    enabled: true,
  });

  const markDirty = React.useCallback(() => markAsChanged(), [markAsChanged]);

  // Intercept navigation attempts
  React.useEffect(() => {
    const handleClick = (event: Event) => {
      const target = event.target as HTMLElement;
      const link = target.closest('a[href]') as HTMLAnchorElement;
      
      if (link && hasUnsavedChanges) {
        const href = link.getAttribute('href');
        if (href && href.startsWith('/') && href !== location.pathname) {
          event.preventDefault();
          setPendingNavigation(href);
          handleClose(); // This will show the unsaved changes dialog
        }
      }
    };

    document.addEventListener('click', handleClick);
    return () => document.removeEventListener('click', handleClick);
  }, [hasUnsavedChanges, location.pathname, handleClose]);

  type SerializableStudent = Pick<Student, 'id' | 'student_id' | 'firstname' | 'surname' | 'program' | 'year' | 'section'>;
  type SerializableTraining = { name: string; type: string; size: number; dataUrl: string };

  const fileToDataUrl = (file: File) => new Promise<string>((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = () => resolve(reader.result as string);
    reader.onerror = reject;
    reader.readAsDataURL(file);
  });

  const dataUrlToFile = (dataUrl: string, name: string, type: string): File => {
    const arr = dataUrl.split(',');
    const bstr = atob(arr[1]);
    let n = bstr.length;
    const u8arr = new Uint8Array(n);
    while (n--) u8arr[n] = bstr.charCodeAt(n);
    return new File([u8arr], name, { type });
  };

  const saveSessionState = async (extra?: { addGenuine?: File[]; addForged?: File[] }) => {
    try {
      // Prepare serializable lists. Append newly added files efficiently.
      const genuineSerial: SerializableTraining[] = [];
      for (const item of genuineFiles) {
        genuineSerial.push({
          name: item.file.name,
          type: item.file.type,
          size: item.file.size,
          dataUrl: await fileToDataUrl(item.file),
        });
      }
      const forgedSerial: SerializableTraining[] = [];
      for (const item of forgedFiles) {
        forgedSerial.push({
          name: item.file.name,
          type: item.file.type,
          size: item.file.size,
          dataUrl: await fileToDataUrl(item.file),
        });
      }
      const studentSerial: SerializableStudent | null = selectedStudent
        ? {
            id: selectedStudent.id,
            student_id: selectedStudent.student_id,
            firstname: selectedStudent.firstname,
            surname: selectedStudent.surname,
            program: selectedStudent.program,
            year: selectedStudent.year,
            section: selectedStudent.section,
          }
        : null;
      const payload = {
        selectedStudent: studentSerial,
        currentTrainingSet,
        visibleCounts,
        genuine: genuineSerial,
        forged: forgedSerial,
        timestamp: Date.now(), // Add timestamp for debugging
      };
      sessionStorage.setItem(STORAGE_KEY, JSON.stringify(payload));
      // Mark that we're navigating internally (not refreshing)
      sessionStorage.setItem(NAVIGATION_FLAG, 'true');
      console.log('State saved successfully', { timestamp: payload.timestamp });
    } catch (e) {
      console.warn('Failed saving session state', e);
    }
  };

  const loadSessionState = async () => {
    try {
      // Check if we have a navigation flag indicating internal navigation
      const hasNavigationFlag = sessionStorage.getItem(NAVIGATION_FLAG);
      
      // Only clear state on actual browser refresh/restart, preserve for internal navigation
      if (!hasNavigationFlag && window.performance.navigation && window.performance.navigation.type === 1) {
        // This is a browser refresh without navigation flag - clear state for fresh start
        console.log('Page reload detected, clearing saved state');
        sessionStorage.removeItem(STORAGE_KEY);
        sessionStorage.removeItem(NAVIGATION_FLAG);
        return;
      }
      const raw = sessionStorage.getItem(STORAGE_KEY);
      if (!raw) {
        console.log('No saved state found');
        return;
      }
      console.log('Loading saved state...');
      const parsed = JSON.parse(raw) as {
        selectedStudent: SerializableStudent | null;
        currentTrainingSet: 'genuine' | 'forged';
        visibleCounts: { genuine: number; forged: number };
        genuine: SerializableTraining[];
        forged: SerializableTraining[];
        timestamp?: number;
      };
      console.log('State loaded successfully', { 
        timestamp: parsed.timestamp,
        studentSelected: !!parsed.selectedStudent,
        genuineCount: parsed.genuine?.length || 0,
        forgedCount: parsed.forged?.length || 0 
      });
      if (parsed.selectedStudent) {
        setSelectedStudent(parsed.selectedStudent as unknown as Student);
      }
      setVisibleCounts(parsed.visibleCounts || { genuine: 60, forged: 60 });
      setCurrentTrainingSet(parsed.currentTrainingSet || 'genuine');
      // Rehydrate files
      const newGenuine: TrainingFile[] = parsed.genuine?.map((s) => {
        const f = dataUrlToFile(s.dataUrl, s.name, s.type);
        return { file: f, preview: URL.createObjectURL(f) };
      }) || [];
      const newForged: TrainingFile[] = parsed.forged?.map((s) => {
        const f = dataUrlToFile(s.dataUrl, s.name, s.type);
        return { file: f, preview: URL.createObjectURL(f) };
      }) || [];
      setGenuineFiles(newGenuine);
      setForgedFiles(newForged);
      
      // Clear navigation flag after successful load
      sessionStorage.removeItem(NAVIGATION_FLAG);
    } catch (e) {
      console.warn('Failed loading session state', e);
    }
  };

  // Warn before unload if there are any interactions/changes
  React.useEffect(() => {
    const handleBeforeUnload = (e: BeforeUnloadEvent) => {
      if (!hasUnsavedChanges) return;
      e.preventDefault();
      e.returnValue = '';
    };
    window.addEventListener('beforeunload', handleBeforeUnload);
    return () => window.removeEventListener('beforeunload', handleBeforeUnload);
  }, [hasUnsavedChanges]);

  // Student Selection State
  const [selectedStudent, setSelectedStudent] = useState<Student | null>(null);
  const [isStudentDialogOpen, setIsStudentDialogOpen] = useState(false);
  const [studentSearch, setStudentSearch] = useState('');
  const [isStudentCollapsed, setIsStudentCollapsed] = useState(true);
  const [debouncedStudentSearch, setDebouncedStudentSearch] = useState('');
  const [isStudentSearching, setIsStudentSearching] = useState(false);
  const [allStudents, setAllStudents] = useState<Student[]>([]);
  const [isLoadingStudents, setIsLoadingStudents] = useState(false);
  
  // Student switch confirmation state
  const [isStudentSwitchConfirmOpen, setIsStudentSwitchConfirmOpen] = useState(false);
  const [pendingStudent, setPendingStudent] = useState<Student | null>(null);

  // Fetch students on component mount
  React.useEffect(() => {
    const loadStudents = async () => {
      setIsLoadingStudents(true);
      try {
        const students = await fetchStudents();
        // Sort alphabetically by name (firstname + surname)
        const sortedStudents = students.sort((a, b) => {
          const nameA = `${a.firstname} ${a.surname}`.toLowerCase();
          const nameB = `${b.firstname} ${b.surname}`.toLowerCase();
          return nameA.localeCompare(nameB);
        });
        setAllStudents(sortedStudents);
      } catch (error) {
        console.error('Error loading students:', error);
        toast({
          title: "Error",
          description: "Failed to load students",
          variant: "destructive",
        });
      } finally {
        setIsLoadingStudents(false);
      }
    };
    loadStudents();
  }, []);

  // Restore persisted state on mount
  React.useEffect(() => {
    loadSessionState();
  }, []);

  // Persist on critical state changes
  React.useEffect(() => {
    saveSessionState();
  }, [selectedStudent, genuineFiles, forgedFiles, currentTrainingSet, visibleCounts]);

  // Ensure state is persisted on internal navigation or tab hide
  React.useEffect(() => {
    const handleVisibilityChange = () => {
      if (document.visibilityState === 'hidden') {
        // Best-effort save before the page is backgrounded or navigating away
        saveSessionState();
      }
    };
    document.addEventListener('visibilitychange', handleVisibilityChange);
    return () => {
      document.removeEventListener('visibilitychange', handleVisibilityChange);
      // Save once more on unmount to capture any last-moment changes
      saveSessionState();
    };
  }, [selectedStudent, genuineFiles, forgedFiles, currentTrainingSet, visibleCounts]);

  React.useEffect(() => {
    setIsStudentSearching(true);
    const t = setTimeout(() => {
      setDebouncedStudentSearch(studentSearch.trim());
      setIsStudentSearching(false);
    }, 300);
    return () => clearTimeout(t);
  }, [studentSearch]);

  const filteredStudents = debouncedStudentSearch 
    ? allStudents.filter((s) => (
        s.student_id.includes(debouncedStudentSearch) ||
        `${s.firstname} ${s.surname}`.toLowerCase().includes(debouncedStudentSearch.toLowerCase())
      ))
    : allStudents.slice(0, 5); // Show first 5 by default
  
  const verificationInputRef = useRef<HTMLInputElement>(null);
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);

  // Validation functions
  const hasUploadedImages = () => {
    return genuineFiles.length > 0 || forgedFiles.length > 0;
  };

  const canTrainModel = () => {
    return selectedStudent !== null && hasUploadedImages();
  };

  // Data Quality Functions
  const getDataBalance = () => {
    const genuineCount = genuineFiles.length;
    const forgedCount = forgedFiles.length;
    const total = genuineCount + forgedCount;
    
    if (total === 0) return { ratio: 0, status: 'none', message: 'No data uploaded' };
    
    const genuineRatio = genuineCount / total;
    const forgedRatio = forgedCount / total;
    
    if (genuineRatio === 0 || forgedRatio === 0) {
      return { ratio: 0, status: 'unbalanced', message: 'Need both genuine and forged samples' };
    }
    
    const ratio = Math.min(genuineRatio, forgedRatio) / Math.max(genuineRatio, forgedRatio);
    
    if (ratio >= 0.7) {
      return { ratio, status: 'balanced', message: 'Good data balance' };
    } else if (ratio >= 0.4) {
      return { ratio, status: 'fair', message: 'Moderate imbalance - consider adding more samples' };
    } else {
      return { ratio, status: 'poor', message: 'Poor balance - add more samples of the minority class' };
    }
  };

  const getImageQualityScore = (file: File): number => {
    // Basic quality assessment based on file size and type
    const sizeScore = Math.min(file.size / (1024 * 1024), 5) / 5; // Max 5MB = 1.0
    const typeScore = ['image/png', 'image/jpeg'].includes(file.type) ? 1.0 : 0.5;
    return (sizeScore + typeScore) / 2;
  };

  const getOverallDataQuality = () => {
    const balance = getDataBalance();
    const totalImages = genuineFiles.length + forgedFiles.length;
    
    if (totalImages === 0) {
      return { score: 0, status: 'poor', message: 'No training data available' };
    }
    
    if (totalImages < 10) {
      return { score: 0.3, status: 'poor', message: 'Very few samples - recommend at least 10 images' };
    }
    
    if (totalImages < 20) {
      return { score: 0.6, status: 'fair', message: 'Limited samples - more data would improve accuracy' };
    }
    
    if (balance.status === 'balanced' && totalImages >= 20) {
      return { score: 0.9, status: 'excellent', message: 'Good training dataset' };
    }
    
    return { score: 0.7, status: 'good', message: 'Decent training dataset' };
  };

  const getTrainModelErrorMessage = () => {
    if (!selectedStudent && !hasUploadedImages()) {
      return "Please select a student and upload at least one signature image to train the model.";
    }
    if (!selectedStudent) {
      return "Please select a student to train the model.";
    }
    if (!hasUploadedImages()) {
      return "Please upload at least one signature image to train the model.";
    }
    return "";
  };

  const handleStudentSelection = (student: Student) => {
    // Check if we have uploaded images and a different student is selected
    if (hasUploadedImages() && selectedStudent && selectedStudent.id !== student.id) {
      setPendingStudent(student);
      setIsStudentSwitchConfirmOpen(true);
    } else {
      setSelectedStudent(student);
      setIsStudentDialogOpen(false);
    }
  };

  const confirmStudentSwitch = () => {
    // Clear all uploaded images
    genuineFiles.forEach(file => URL.revokeObjectURL(file.preview));
    forgedFiles.forEach(file => URL.revokeObjectURL(file.preview));
    setGenuineFiles([]);
    setForgedFiles([]);
    
    // Set new student
    setSelectedStudent(pendingStudent);
    setIsStudentDialogOpen(false);
    setIsStudentSwitchConfirmOpen(false);
    setPendingStudent(null);
    markDirty();
    
    toast({
      title: "Student Changed",
      description: "All uploaded images have been cleared for the new student.",
    });
  };

  const cancelStudentSwitch = () => {
    setIsStudentSwitchConfirmOpen(false);
    setPendingStudent(null);
  };

  // Training Functions
  const validateFiles = (files: File[]): File[] => {
    const MAX_SIZE_BYTES = 5 * 1024 * 1024; // 5MB
    const valid: File[] = [];
    let rejected = 0;
    files.forEach((f) => {
      // Accept any file that starts with 'image/' to support all common image formats
      if (!f.type.startsWith('image/')) {
        rejected++;
        return;
      }
      if (f.size > MAX_SIZE_BYTES) {
        rejected++;
        return;
      }
      valid.push(f);
    });
    if (rejected > 0) {
      toast({
        title: 'Some files were ignored',
        description: 'Only image files up to 5MB are allowed.',
        variant: 'destructive',
      });
    }
    return valid;
  };

  const handleTrainingFilesChange = (files: File[], setType: 'genuine' | 'forged') => {
    const safeFiles = validateFiles(files);
    const newFiles = safeFiles.map(file => ({
      file,
      preview: URL.createObjectURL(file)
    }));
    if (setType === 'genuine') {
      setGenuineFiles(prev => [...prev, ...newFiles]);
    } else {
      setForgedFiles(prev => [...prev, ...newFiles]);
    }
    markDirty();
  };

  const removeTrainingFile = (index: number, setType: 'genuine' | 'forged') => {
    if (setType === 'genuine') {
      setGenuineFiles(prev => {
        const newFiles = [...prev];
        URL.revokeObjectURL(newFiles[index].preview);
        newFiles.splice(index, 1);
        return newFiles;
      });
    } else {
      setForgedFiles(prev => {
        const newFiles = [...prev];
        URL.revokeObjectURL(newFiles[index].preview);
        newFiles.splice(index, 1);
        return newFiles;
      });
    }
    markDirty();
  };

  const handleTrainModel = async () => {
    if (!canTrainModel()) {
      toast({
        title: "Cannot Train Model",
        description: getTrainModelErrorMessage(),
        variant: "destructive",
      });
      return;
    }

    setIsTraining(true);
    setTrainingResult(null);
    setTrainingProgress(0);
    setTrainingStage('training');
    setEstimatedTimeRemaining('');

    try {
      // Start async training job
      const asyncResponse = await aiService.startAsyncTraining(
        selectedStudent.student_id,
        genuineFiles.map(g => g.file),
        forgedFiles.map(f => f.file)
      );
      
      // Subscribe to real-time progress updates
      const eventSource = aiService.subscribeToJobProgress(
        asyncResponse.job_id,
        (job) => {
          setTrainingProgress(job.progress);
          setTrainingStatus(job.current_stage);
          
          if (job.estimated_time_remaining) {
            const minutes = Math.floor(job.estimated_time_remaining / 60);
            const seconds = job.estimated_time_remaining % 60;
            setEstimatedTimeRemaining(`~${minutes}:${seconds.toString().padStart(2, '0')} remaining`);
          }
          
          if (job.status === 'completed') {
            setTrainingStage('completed');
            setTrainingStatus('Training completed!');
            setEstimatedTimeRemaining('');
            setTrainingResult(job.result);
            
            toast({
              title: "Training Completed",
              description: "AI model has been successfully trained for this student",
            });
            
            eventSource.close();
            setIsTraining(false);
          } else if (job.status === 'failed') {
            setTrainingStage('error');
            setTrainingStatus('Training failed');
            setTrainingProgress(0);
            
            toast({
              title: "Training Failed",
              description: job.error || "Failed to complete training",
              variant: "destructive",
            });
            
            eventSource.close();
            setIsTraining(false);
          }
        },
        (error) => {
          console.error('Training progress error:', error);
          setTrainingStage('error');
          setTrainingStatus('Connection error');
          setTrainingProgress(0);
          
          toast({
            title: "Connection Error",
            description: "Lost connection to training progress updates",
            variant: "destructive",
          });
          
          eventSource.close();
          setIsTraining(false);
        }
      );
      
    } catch (error) {
      console.error('Training error:', error);
      setTrainingStage('error');
      setTrainingStatus('Training failed');
      setTrainingProgress(0);
      toast({
        title: "Error",
        description: "An unexpected error occurred during training",
        variant: "destructive",
      });
      setIsTraining(false);
    }
  };

  // Verification Functions
  const handleVerificationFileChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    const raw = e.target.files?.[0];
    const valid = raw ? validateFiles([raw]) : [];
    const file = valid[0];
    if (file) {
      setVerificationFile(file);
      setVerificationPreview(URL.createObjectURL(file));
      setVerificationResult(null);
      markDirty();
    }
  };

  // Reference files are optional UI-wise; if none provided, we'll fall back to
  // the currently uploaded Genuine training files for the selected student.

  const startCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        setUseCamera(true);
        markDirty();
      }
    } catch (error) {
      console.error('Error accessing camera:', error);
      toast({
        title: "Camera Error",
        description: "Unable to access camera. Please check permissions.",
        variant: "destructive",
      });
    }
  };

  const capturePhoto = () => {
    if (videoRef.current && canvasRef.current) {
      const canvas = canvasRef.current;
      const video = videoRef.current;
      const context = canvas.getContext('2d');
      
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      
      if (context) {
        context.drawImage(video, 0, 0);
        canvas.toBlob((blob) => {
          if (blob) {
            const file = new File([blob], 'signature.png', { type: 'image/png' });
            setVerificationFile(file);
            setVerificationPreview(URL.createObjectURL(file));
            setUseCamera(false);
            setVerificationResult(null);
            markDirty();
            
            // Stop camera stream
            const stream = video.srcObject as MediaStream;
            stream?.getTracks().forEach(track => track.stop());
            // Auto-trigger verification on capture
            setTimeout(() => {
              handleVerifySignature();
            }, 0);
          }
        });
      }
    }
  };

  const stopCamera = () => {
    if (videoRef.current) {
      const stream = videoRef.current.srcObject as MediaStream;
      stream?.getTracks().forEach(track => track.stop());
      setUseCamera(false);
      markDirty();
    }
  };

  // Modal Functions
  type ModalContext = { kind: 'training', setType: 'genuine' | 'forged' } | { kind: 'verification' } | null;
  const [modalContext, setModalContext] = useState<ModalContext>(null);

  const openImageModal = (images: string[], startIndex: number = 0, context: ModalContext = null) => {
    setModalImages(images);
    setModalImageIndex(startIndex);
    setIsModalOpen(true);
    setModalContext(context);
  };

  const closeImageModal = () => {
    setIsModalOpen(false);
    setModalImages([]);
    setModalImageIndex(0);
  };

  const goToPreviousImage = () => {
    setModalImageIndex(prev => prev > 0 ? prev - 1 : modalImages.length - 1);
  };

  const goToNextImage = () => {
    setModalImageIndex(prev => prev < modalImages.length - 1 ? prev + 1 : 0);
  };

  const removeAllTrainingFiles = () => {
    genuineFiles.forEach(file => URL.revokeObjectURL(file.preview));
    forgedFiles.forEach(file => URL.revokeObjectURL(file.preview));
    setGenuineFiles([]);
    setForgedFiles([]);
    setIsDropdownOpen(false);
    toast({
      title: "Samples Removed",
      description: "All training samples have been removed",
    });
    markDirty();
  };

  const deleteModalCurrentImage = () => {
    if (!modalContext || modalContext.kind !== 'training') return;
    const targetPreview = modalImages[modalImageIndex];
    if (modalContext.setType === 'genuine') {
      const idx = genuineFiles.findIndex(f => f.preview === targetPreview);
      if (idx !== -1) removeTrainingFile(idx, 'genuine');
      const updated = genuineFiles.filter(f => f.preview !== targetPreview).map(f => f.preview);
      setModalImages(updated);
    } else {
      const idx = forgedFiles.findIndex(f => f.preview === targetPreview);
      if (idx !== -1) removeTrainingFile(idx, 'forged');
      const updated = forgedFiles.filter(f => f.preview !== targetPreview).map(f => f.preview);
      setModalImages(updated);
    }
    setModalImageIndex(prev => Math.max(0, prev - (modalImages.length === 1 ? 0 : 1)));
    if (modalImages.length <= 1) closeImageModal();
  };

  const handleVerifySignature = async () => {
    if (!verificationFile) {
      toast({
        title: "Error",
        description: "Please upload or capture a signature image",
        variant: "destructive",
      });
      return;
    }

    if (!selectedStudent) {
      toast({
        title: "Error",
        description: "Select a student first",
        variant: "destructive",
      });
      return;
    }

    // Build reference set: use explicit referenceFiles if provided, otherwise
    // fall back to Genuine training files currently uploaded in the UI.
    const effectiveReferences: File[] =
      referenceFiles.length > 0 ? referenceFiles : genuineFiles.map(g => g.file);
    if (effectiveReferences.length === 0) {
      toast({
        title: "Reference Required",
        description: "Upload some Genuine samples (left pane) or add references to verify",
        variant: "destructive",
      });
      return;
    }

    setIsVerifying(true);
    setVerificationResult(null);

    try {
      // Auto-select the model linked to the selected student
      // We'll fetch trained models filtered by DB numeric id
      const models = await aiService.getTrainedModels(selectedStudent.id);
      const readyModel = models.find((m: any) => m.status === 'completed');
      if (!readyModel) {
        toast({
          title: "No Trained Model",
          description: "No trained model found for this student",
          variant: "destructive",
        });
        setIsVerifying(false);
        return;
      }

      // Prototype-based verify: only student_id + test_file
      const form = new FormData();
      form.append('student_id', String(selectedStudent.student_id));
      form.append('test_file', verificationFile);
      const resp = await fetch(`${AI_CONFIG.BASE_URL}/api/verification/verify`, { method: 'POST', body: form });
      const result = await resp.json();
      setVerificationResult(result);
      
      if (result.success) {
        toast({
          title: "Verification Complete",
          description: `Result: ${result.match ? 'Match found' : 'No match'}`,
        });
      } else {
        toast({
          title: "Verification Failed",
          description: result.message || "Failed to verify signature",
          variant: "destructive",
        });
      }
    } catch (error) {
      console.error('Verification error:', error);
      toast({
        title: "Error",
        description: "An unexpected error occurred during verification",
        variant: "destructive",
      });
    } finally {
      setIsVerifying(false);
    }
  };

  return (
    <Layout>
      <div
        className="flex-1 flex flex-col space-y-6 px-6 py-4"
        onClick={markDirty}
        onInput={markDirty}
      >
        {/* Page Header */}
        <div className="space-y-0.5">
          <h1 className="text-lg font-bold text-education-navy">SIGNATURE AI</h1>
          <p className="text-sm text-muted-foreground">
            Train AI models and verify signatures using machine learning
          </p>
        </div>

        {/* Student Selection Card */}
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
          <Card className="h-fit">
          <CardHeader className="py-3">
            {/* Collapsed header */}
            {isStudentCollapsed ? (
              <div className="flex items-center justify-between">
                <div className="flex items-center gap-2 text-left">
                  <span className="text-sm font-medium">
                    {selectedStudent ? `${selectedStudent.firstname} ${selectedStudent.surname}` : 'No student selected'}
                  </span>
                </div>
                <div className="flex items-center gap-1">
                  <Button
                    variant="ghost"
                    size="sm"
                    className={`h-6 w-6 p-0 opacity-60 text-muted-foreground hover:opacity-100 hover:text-foreground hover:bg-transparent transition-transform ${isStudentCollapsed ? 'rotate-180' : ''}`}
                    onClick={() => setIsStudentCollapsed(false)}
                    aria-label="Expand"
                  >
                    <ChevronDown className="h-4 w-4" />
                  </Button>
                  <DropdownMenu>
                    <DropdownMenuTrigger asChild>
                      <Button
                        variant="ghost"
                        size="sm"
                        className="h-6 w-6 p-0 opacity-60 text-muted-foreground hover:opacity-100 hover:text-foreground hover:bg-transparent transition-colors"
                      >
                        <MoreVertical className="h-4 w-4" />
                      </Button>
                    </DropdownMenuTrigger>
                    <DropdownMenuContent align="end">
                      <DropdownMenuItem onClick={() => setIsStudentDialogOpen(true)}>
                        {selectedStudent ? 'Change' : 'Select'}
                      </DropdownMenuItem>
                    </DropdownMenuContent>
                  </DropdownMenu>
                </div>
              </div>
            ) : (
              // Expanded header
              <div className="flex items-start justify-between">
                <div className="space-y-1">
                  <CardTitle className="flex items-center gap-2">
                    Select Student
                  </CardTitle>
                  <CardDescription>
                    Choose a student to train the AI model for signature verification
                  </CardDescription>
                </div>
                <div className="flex items-center gap-1">
                  <Button
                    variant="ghost"
                    size="sm"
                    className={`h-6 w-6 p-0 opacity-60 text-muted-foreground hover:opacity-100 hover:text-foreground hover:bg-transparent transition-transform ${isStudentCollapsed ? 'rotate-180' : ''}`}
                    onClick={() => setIsStudentCollapsed(true)}
                    aria-label="Collapse"
                  >
                    <ChevronDown className="h-4 w-4" />
                  </Button>
                  <DropdownMenu>
                    <DropdownMenuTrigger asChild>
                      <Button
                        variant="ghost"
                        size="sm"
                        className="h-6 w-6 p-0 opacity-60 text-muted-foreground hover:opacity-100 hover:text-foreground hover:bg-transparent transition-colors"
                      >
                        <MoreVertical className="h-4 w-4" />
                      </Button>
                    </DropdownMenuTrigger>
                    <DropdownMenuContent align="end">
                      <DropdownMenuItem onClick={() => setIsStudentDialogOpen(true)}>
                        {selectedStudent ? 'Change' : 'Select'}
                      </DropdownMenuItem>
                    </DropdownMenuContent>
                  </DropdownMenu>
                </div>
              </div>
            )}
          </CardHeader>
          {/* Collapsible content */}
          <div className={`overflow-hidden transition-all ${isStudentCollapsed ? 'max-h-0 py-0' : 'max-h-[500px]'}`}>
            {!isStudentCollapsed && (
              <CardContent>
                {selectedStudent ? (
                  <div className="grid grid-cols-1 sm:grid-cols-2 gap-4 max-w-2xl">
                    <div>
                      <Label className="text-muted-foreground">ID</Label>
                      <div className="font-medium">{selectedStudent?.student_id ?? '—'}</div>
                    </div>
                    <div>
                      <Label className="text-muted-foreground">Name</Label>
                      <div className="font-medium">{selectedStudent ? `${selectedStudent.firstname} ${selectedStudent.surname}` : '—'}</div>
                    </div>
                    <div>
                      <Label className="text-muted-foreground">Program</Label>
                      <div className="font-medium">{selectedStudent?.program ?? '—'}</div>
                    </div>
                    <div>
                      <Label className="text-muted-foreground">Year</Label>
                      <div className="font-medium">{selectedStudent?.year ?? '—'}</div>
                    </div>
                    <div>
                      <Label className="text-muted-foreground">Section</Label>
                      <div className="font-medium">{selectedStudent?.section ?? '—'}</div>
                    </div>
                  </div>
                ) : (
                  <div className="text-sm text-muted-foreground">No student selected.</div>
                )}
              </CardContent>
            )}
          </div>
          </Card>
        </div>

        {/* Main Content - Two Cards Side by Side */}
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
          
          {/* Model Training Section */}
          <Card className="h-fit">
            <CardHeader>
              <div className="flex items-start justify-between">
                <div className="space-y-1">
                  <CardTitle className="flex items-center gap-2">
                    <Brain className="w-5 h-5" />
                    Model Training
                  </CardTitle>
                  <CardDescription>
                    Upload signature samples to train AI model for a specific student
                  </CardDescription>
                </div>
                <DropdownMenu open={isDropdownOpen} onOpenChange={setIsDropdownOpen}>
                  <DropdownMenuTrigger asChild>
                    <Button 
                      variant="ghost" 
                      size="sm" 
                      className="h-6 w-6 p-0 opacity-60 text-muted-foreground hover:opacity-100 hover:text-foreground hover:bg-transparent transition-colors"
                    >
                      <MoreVertical className="h-4 w-4" />
                    </Button>
                  </DropdownMenuTrigger>
                  <DropdownMenuContent align="end">
                    <DropdownMenuItem 
                      onClick={() => setIsConfirmRemoveOpen(true)} 
                      className="text-red-600"
                      disabled={(genuineFiles.length + forgedFiles.length) === 0}
                    >
                      Remove All Samples
                    </DropdownMenuItem>
                  </DropdownMenuContent>
                </DropdownMenu>
              </div>
            </CardHeader>
            <CardContent className="space-y-4">
              {/* Upload Buttons: Forged (left, normal) and Genuine (right, highlighted) */}
              <div className="flex gap-2">
                <Button
                  variant="outline"
                  size="sm"
                  className="flex items-center gap-2 hover:bg-transparent hover:text-foreground"
                  onClick={() => {
                    setCurrentTrainingSet('forged');
                    const input = document.createElement('input');
                    input.type = 'file';
                    input.accept = 'image/*';
                    input.multiple = true;
                    input.onchange = (e) => {
                      const files = Array.from((e.target as HTMLInputElement).files || []);
                      handleTrainingFilesChange(files, 'forged');
                    };
                    input.click();
                  }}
                >
                  <Upload className="w-4 h-4" />
                  Forged
                </Button>
                <Button
                  variant="default"
                  size="sm"
                  className="flex items-center gap-2"
                  onClick={() => {
                    setCurrentTrainingSet('genuine');
                    const input = document.createElement('input');
                    input.type = 'file';
                    input.accept = 'image/*';
                    input.multiple = true;
                    input.onchange = (e) => {
                      const files = Array.from((e.target as HTMLInputElement).files || []);
                      handleTrainingFilesChange(files, 'genuine');
                    };
                    input.click();
                  }}
                >
                  <Upload className="w-4 h-4" />
                  Genuine
                </Button>
              </div>

              {/* Large Square Preview Box for Training Images (Genuine/Forged switch) */}
              <div className="space-y-2">
                <div className="flex items-center justify-between">
                  <Label>Training Images Preview</Label>
                  <div className="text-xs text-muted-foreground">
                    {currentTrainingSet === 'genuine' ? (
                      <span>Genuine ({genuineFiles.length})</span>
                    ) : (
                      <span>Forged ({forgedFiles.length})</span>
                    )}
                  </div>
                </div>
                <div className="relative w-full h-64 border-2 border-dashed border-gray-300 rounded-lg flex items-center justify-center bg-gray-50 group">
                  {/* Hover Previous/Next inside box */}
                  <button
                    className="hidden group-hover:flex absolute left-2 top-1/2 -translate-y-1/2 bg-white/80 hover:bg-white rounded-full p-2 shadow"
                    onClick={() => setCurrentTrainingSet(prev => prev === 'genuine' ? 'forged' : 'genuine')}
                    aria-label="Previous"
                  >
                    <ChevronLeft className="w-4 h-4" />
                  </button>
                  <button
                    className="hidden group-hover:flex absolute right-2 top-1/2 -translate-y-1/2 bg-white/80 hover:bg-white rounded-full p-2 shadow"
                    onClick={() => setCurrentTrainingSet(prev => prev === 'genuine' ? 'forged' : 'genuine')}
                    aria-label="Next"
                  >
                    <ChevronRight className="w-4 h-4" />
                  </button>

                  {((currentTrainingSet === 'genuine' ? genuineFiles : forgedFiles).length > 0) ? (
                    <div className="grid grid-cols-3 gap-2 w-full h-full p-4 overflow-y-auto">
                      {(currentTrainingSet === 'genuine' ? genuineFiles : forgedFiles)
                        .slice(0, visibleCounts[currentTrainingSet])
                        .map((item, index) => (
                        <div key={index} className="relative group/itm cursor-pointer" onClick={() => openImageModal((currentTrainingSet === 'genuine' ? genuineFiles : forgedFiles).map(f => f.preview), index, { kind: 'training', setType: currentTrainingSet })}>
                          <img
                            src={item.preview}
                            alt={`Sample ${index + 1}`}
                            className="w-full h-16 object-cover rounded border hover:opacity-80 transition-opacity"
                            loading="lazy"
                          />
                          <Button
                            size="sm"
                            variant="destructive"
                            className="absolute top-1 right-1 w-6 h-6 p-0 opacity-0 group-hover/itm:opacity-100 transition-opacity flex items-center justify-center"
                            onClick={(e) => {
                              e.stopPropagation();
                              removeTrainingFile(index, currentTrainingSet);
                            }}
                          >
                            <Trash2 className="w-3.5 h-3.5" />
                          </Button>
                        </div>
                      ))}
                      {(currentTrainingSet === 'genuine' ? genuineFiles.length : forgedFiles.length) > visibleCounts[currentTrainingSet] && (
                        <div className="col-span-3 flex justify-center pt-2">
                          <button
                            className="text-xs text-foreground hover:underline"
                            onClick={() =>
                              setVisibleCounts((prev) => ({
                                ...prev,
                                [currentTrainingSet]: prev[currentTrainingSet] + 60,
                              }))
                            }
                          >
                            Show more
                          </button>
                        </div>
                      )}
                    </div>
                  ) : (
                    <div className="text-center text-gray-500">
                      <Upload className="w-8 h-8 mx-auto mb-2" />
                      <p>No {currentTrainingSet === 'genuine' ? 'genuine' : 'forged'} images uploaded</p>
                    </div>
                  )}
                </div>
              </div>





              {/* Train Button */}
              <Button
                onClick={handleTrainModel}
                disabled={!canTrainModel() || isTraining}
                className="w-full"
              >
                {isTraining ? (
                  <>
                    <Loader2 className="w-4 h-4 mr-2 animate-spin" />
                    {trainingStatus} {Math.round(trainingProgress)}%
                  </>
                ) : (
                  <>
                    <Brain className="w-4 h-4 mr-2" />
                    Train Model
                  </>
                )}
              </Button>

              {/* Training Results */}
              {trainingResult && (
                <div className="space-y-3">
                  <Alert className={trainingResult.success ? "border-green-200 bg-green-50" : "border-red-200 bg-red-50"}>
                    <div className="flex items-center gap-2">
                      {trainingResult.success ? (
                        <CheckCircle className="w-4 h-4 text-green-600" />
                      ) : (
                        <AlertCircle className="w-4 h-4 text-red-600" />
                      )}
                      <AlertDescription>
                        <div className="space-y-2">
                          <p>
                            <strong>Status:</strong> {trainingResult.success ? 'Training Completed' : 'Training Failed'}
                          </p>
                          {trainingResult.profile && (
                            <>
                              <p>
                                <strong>Model Status:</strong>{' '}
                                <Badge variant={trainingResult.profile.status === 'ready' ? 'default' : 'secondary'}>
                                  {trainingResult.profile.status}
                                </Badge>
                              </p>
                              <p>
                                <strong>Samples:</strong> {trainingResult.profile.num_samples}
                              </p>
                              {trainingResult.profile.last_trained_at && (
                                <p>
                                  <strong>Last Trained:</strong>{' '}
                                  {new Date(trainingResult.profile.last_trained_at).toLocaleString()}
                                </p>
                              )}
                            </>
                          )}
                          <p className="text-sm text-muted-foreground">
                            {trainingResult.message}
                          </p>
                        </div>
                    </AlertDescription>
                  </div>
                </Alert>
                
                {/* Training Metrics */}
                {trainingResult.success && (
                  <div className="p-4 bg-green-50 border border-green-200 rounded-lg">
                    <h4 className="text-sm font-medium text-green-900 mb-3">Training Metrics</h4>
                    <div className="grid grid-cols-2 gap-4 text-xs">
                      <div className="space-y-2">
                        <div className="flex justify-between">
                          <span className="text-green-700">Train Acc:</span>
                          <span className="font-medium text-green-800">{typeof trainingResult.accuracy === 'number' ? trainingResult.accuracy.toFixed(3) : '—'}</span>
                        </div>
                        <div className="flex justify-between">
                          <span className="text-green-700">Val Acc:</span>
                          <span className="font-medium text-green-800">{typeof trainingResult.val_accuracy === 'number' ? trainingResult.val_accuracy.toFixed(3) : '—'}</span>
                        </div>
                        <div className="flex justify-between">
                          <span className="text-green-700">Precision:</span>
                          <span className="font-medium text-green-800">{typeof trainingResult.precision === 'number' ? trainingResult.precision.toFixed(3) : '—'}</span>
                        </div>
                      </div>
                      <div className="space-y-2">
                        <div className="flex justify-between">
                          <span className="text-green-700">Recall:</span>
                          <span className="font-medium text-green-800">{typeof trainingResult.recall === 'number' ? trainingResult.recall.toFixed(3) : '—'}</span>
                        </div>
                        <div className="flex justify-between">
                          <span className="text-green-700">F1 Score:</span>
                          <span className="font-medium text-green-800">{typeof trainingResult.f1 === 'number' ? trainingResult.f1.toFixed(3) : '—'}</span>
                        </div>
                        <div className="flex justify-between">
                          <span className="text-green-700">Training Time:</span>
                          <span className="font-medium text-green-800">{typeof trainingResult.train_time_s === 'number' ? `${trainingResult.train_time_s}s` : '—'}</span>
                        </div>
                      </div>
                    </div>
                    <div className="mt-3 text-xs text-green-700">
                      Model is ready for signature verification!
                      {trainingResult.calibration && (
                        <div className="mt-2 grid grid-cols-3 gap-2">
                          <div><strong>Thr:</strong> {typeof trainingResult.calibration.threshold === 'number' ? trainingResult.calibration.threshold.toFixed(4) : '—'}</div>
                          <div><strong>FAR:</strong> {typeof trainingResult.calibration.far === 'number' ? trainingResult.calibration.far.toFixed(3) : '—'}</div>
                          <div><strong>FRR:</strong> {typeof trainingResult.calibration.frr === 'number' ? trainingResult.calibration.frr.toFixed(3) : '—'}</div>
                        </div>
                      )}
                      <div className="mt-2 p-2 bg-blue-50 border border-blue-200 rounded text-blue-800">
                        <strong>Data Augmentation Applied:</strong> Rotation, scale, brightness, blur, and thickness variations for improved robustness
                      </div>
                    </div>
                  </div>
                )}
                </div>
              )}
            </CardContent>
          </Card>

          {/* Signature Verification Section */}
          <Card className="h-fit">
            <CardHeader>
              <div className="flex items-start justify-between">
                <div className="space-y-1">
                  <CardTitle className="flex items-center gap-2">
                    <Scan className="w-5 h-5" />
                    Signature Verification
                  </CardTitle>
                  <CardDescription>
                    Upload or capture a signature to verify against trained models
                  </CardDescription>
                </div>
                <DropdownMenu>
                  <DropdownMenuTrigger asChild>
                    <Button
                      variant="ghost"
                      size="sm"
                      className="h-6 w-6 p-0 opacity-60 text-muted-foreground hover:opacity-100 hover:text-foreground hover:bg-transparent transition-colors"
                    >
                      <MoreVertical className="h-4 w-4" />
                    </Button>
                  </DropdownMenuTrigger>
                  <DropdownMenuContent align="end">
                    <DropdownMenuItem
                      onClick={() => {
                        setVerificationFile(null);
                        setVerificationPreview('');
                        stopCamera();
                      }}
                      disabled={!verificationFile && !useCamera && !verificationPreview}
                    >
                      Clear
                    </DropdownMenuItem>
                  </DropdownMenuContent>
                </DropdownMenu>
              </div>
            </CardHeader>
            <CardContent className="space-y-4">
              {/* Camera/Upload Toggle */}
              <div className="flex gap-2">
                <Button
                  variant={useCamera ? "default" : "outline"}
                  size="sm"
                  onClick={startCamera}
                  className="flex items-center gap-2 hover:bg-transparent hover:text-foreground"
                >
                  <Camera className="w-4 h-4" />
                  Camera
                </Button>
                <Button
                  variant={!useCamera ? "default" : "outline"}
                  size="sm"
                  onClick={() => {
                    setUseCamera(false);
                    stopCamera();
                    verificationInputRef.current?.click();
                  }}
                  className="flex items-center gap-2"
                >
                  <Upload className="w-4 h-4" />
                  Upload
                </Button>
              </div>

              {/* Large Square Preview Box */}
              <div className="space-y-2">
                <div className="flex items-center justify-between">
                  <Label>Signature Preview</Label>
                  <div className="text-xs text-muted-foreground">{useCamera ? 'Camera' : 'Upload'}</div>
                </div>
                <div className="w-full h-64 border-2 border-dashed border-gray-300 rounded-lg flex items-center justify-center bg-gray-50">
                  {useCamera ? (
                    <div className="w-full h-full">
                      <video
                        ref={videoRef}
                        autoPlay
                        playsInline
                        className="w-full h-full object-cover rounded-lg"
                      />
                    </div>
                  ) : verificationPreview ? (
                    <img
                      src={verificationPreview}
                      alt="Signature preview"
                      className="w-full h-full object-contain rounded-lg cursor-pointer hover:opacity-80 transition-opacity"
                      onClick={() => openImageModal([verificationPreview], 0)}
                    />
                  ) : (
                    <div className="text-center text-gray-500">
                      <Upload className="w-8 h-8 mx-auto mb-2" />
                      <p>No signature selected</p>
                    </div>
                  )}
                </div>
              </div>

              {/* Camera Controls */}
              {useCamera && (
                <div className="flex gap-2">
                  <Button onClick={capturePhoto} size="sm" className="flex-1">
                    Capture
                  </Button>
                  <Button onClick={stopCamera} variant="outline" size="sm" className="flex-1">
                    Cancel
                  </Button>
                </div>
              )}

              {/* Hidden File Input */}
              <Input
                ref={verificationInputRef}
                type="file"
                accept="image/*"
                onChange={handleVerificationFileChange}
                className="hidden"
              />

              {/* Verify Button */}
              <Button
                onClick={handleVerifySignature}
                disabled={!verificationFile || isVerifying}
                className="w-full"
              >
                {isVerifying ? (
                  <>
                    <Loader2 className="w-4 h-4 mr-2 animate-spin" />
                    Verifying...
                  </>
                ) : (
                  <>
                    <Scan className="w-4 h-4 mr-2" />
                    Verify Signature
                  </>
                )}
              </Button>

              {/* Reference images are optional now; if not provided,
                 Genuine training uploads will be used for verification */}

              {/* Verification Result */}
              {verificationResult && (
                <Alert className={verificationResult.match ? "border-green-200 bg-green-50" : "border-red-200 bg-red-50"}>
                  <div className="flex items-center gap-2">
                    {verificationResult.match ? (
                      <CheckCircle className="w-4 h-4 text-green-600" />
                    ) : (
                      <XCircle className="w-4 h-4 text-red-600" />
                    )}
                    <AlertDescription>
                      <div className="space-y-2">
                        <p>
                          <strong>Result:</strong> {verificationResult.match ? 'Match Found' : 'No Match'}
                        </p>
                        <p>
                          <strong>Confidence:</strong> {(verificationResult.score * 100).toFixed(1)}%
                        </p>
                        {verificationResult.predicted_student && (
                          <p>
                            <strong>Predicted Student:</strong> {verificationResult.predicted_student.firstname} {verificationResult.predicted_student.surname}
                          </p>
                        )}
                        <p className="text-sm text-muted-foreground">
                          {verificationResult.message}
                        </p>
                        
                        {/* Anti-Spoofing Warnings */}
                        {verificationResult.antispoofing && (
                          <div className="mt-3 p-3 bg-amber-50 border border-amber-200 rounded-lg">
                            <div className="flex items-start gap-2">
                              <AlertTriangle className="w-4 h-4 text-amber-600 mt-0.5 flex-shrink-0" />
                              <div className="space-y-2 text-sm">
                                <p className="font-medium text-amber-800">
                                  {verificationResult.antispoofing.warning_message}
                                </p>
                                {verificationResult.antispoofing.is_potentially_spoofed && (
                                  <div className="grid grid-cols-2 gap-2 text-xs text-amber-700">
                                    {verificationResult.antispoofing.is_likely_printed && (
                                      <div>
                                        <strong>Printed:</strong> {(verificationResult.antispoofing.printed_confidence * 100).toFixed(1)}%
                                      </div>
                                    )}
                                    {verificationResult.antispoofing.is_low_quality && (
                                      <div>
                                        <strong>Quality:</strong> {(verificationResult.antispoofing.quality_score * 100).toFixed(1)}%
                                      </div>
                                    )}
                                  </div>
                                )}
                              </div>
                            </div>
                          </div>
                        )}
                      </div>
                    </AlertDescription>
                  </div>
                </Alert>
              )}
            </CardContent>
          </Card>
        </div>

        {/* Image Preview Modal */
        }
        <Dialog open={isModalOpen} onOpenChange={setIsModalOpen}>
          <DialogContent className="max-w-4xl max-h-[90vh] p-0">
            <DialogHeader className="p-6 pb-0">
              <DialogTitle>Image Preview</DialogTitle>
            </DialogHeader>
            <div className="relative p-6">
              {modalImages.length > 0 && (
                <>
                  <div className="relative group">
                    <img
                      src={modalImages[modalImageIndex]}
                      alt={`Preview ${modalImageIndex + 1}`}
                      className="w-full h-auto max-h-[60vh] object-contain mx-auto"
                    />
                    {/* Prev/Next Arrows - inside image boundary, show on hover */}
                    <Button
                      variant="ghost"
                      size="icon"
                      className="hidden group-hover:flex absolute left-3 top-1/2 -translate-y-1/2 bg-black/50 hover:bg-black/70 text-white rounded-full w-8 h-8 items-center justify-center"
                      onClick={goToPreviousImage}
                      aria-label="Previous"
                    >
                      <ChevronLeft className="w-4 h-4" />
                    </Button>
                    <Button
                      variant="ghost"
                      size="icon"
                      className="hidden group-hover:flex absolute right-3 top-1/2 -translate-y-1/2 bg-black/50 hover:bg-black/70 text-white rounded-full w-8 h-8 items-center justify-center"
                      onClick={goToNextImage}
                      aria-label="Next"
                    >
                      <ChevronRight className="w-4 h-4" />
                    </Button>
                  </div>

                  {/* Count + Trash inline with translucent background (old style) */}
                  <div className="absolute bottom-4 left-1/2 -translate-x-1/2">
                    <div className="bg-black/50 text-white px-3 py-1 rounded-full text-sm flex items-center gap-3">
                      <span>{modalImageIndex + 1} / {modalImages.length}</span>
                      {modalContext && modalContext.kind === 'training' && (
                        <button onClick={deleteModalCurrentImage} aria-label="Delete Image" className="text-white">
                          <Trash2 className="w-4 h-4" />
                        </button>
                      )}
                    </div>
                  </div>
                </>
              )}
            </div>
          </DialogContent>
        </Dialog>

        {/* Student Selection Dialog */}
        <Dialog open={isStudentDialogOpen} onOpenChange={setIsStudentDialogOpen}>
          <DialogContent className="max-w-xl">
            <DialogHeader>
              <DialogTitle>Select Student</DialogTitle>
            </DialogHeader>
            <div className="space-y-4">
              <Input
                placeholder="Search by ID or Name"
                value={studentSearch}
                onChange={(e) => setStudentSearch(e.target.value)}
              />
              <div className="max-h-64 overflow-auto border rounded-md">
                {isLoadingStudents ? (
                  <div className="p-4 text-sm text-muted-foreground">Loading students…</div>
                ) : isStudentSearching ? (
                  <div className="p-4 text-sm text-muted-foreground">Searching…</div>
                ) : filteredStudents.length === 0 ? (
                  <div className="p-4 text-sm text-muted-foreground">No results</div>
                ) : (
                  <ul className="divide-y">
                    {filteredStudents.map((s) => (
                      <li key={s.id}>
                        <button
                          className="w-full text-left p-3 hover:bg-muted/50"
                          onClick={() => handleStudentSelection(s)}
                        >
                          <div className="font-medium">{`${s.firstname} ${s.surname}`}</div>
                          <div className="text-xs text-muted-foreground">ID: {s.student_id} • {s.program} • Year {s.year} • Sec {s.section}</div>
                        </button>
                      </li>
                    ))}
                  </ul>
                )}
              </div>
            </div>
          </DialogContent>
        </Dialog>
        {/* Confirm Remove All Samples */}
        <Dialog open={isConfirmRemoveOpen} onOpenChange={setIsConfirmRemoveOpen}>
          <DialogContent className="max-w-sm">
            <DialogHeader>
              <DialogTitle>Remove all samples?</DialogTitle>
            </DialogHeader>
            <div className="text-sm text-muted-foreground">
              This action will remove all Genuine and Forged samples. This cannot be undone.
            </div>
            <div className="flex justify-end gap-2 pt-4">
              <Button variant="outline" size="sm" onClick={() => setIsConfirmRemoveOpen(false)}>Cancel</Button>
              <Button variant="destructive" size="sm" onClick={() => { removeAllTrainingFiles(); setIsConfirmRemoveOpen(false); }}>Remove</Button>
            </div>
          </DialogContent>
        </Dialog>

        {/* Student Switch Confirmation Dialog */}
        <Dialog open={isStudentSwitchConfirmOpen} onOpenChange={setIsStudentSwitchConfirmOpen}>
          <DialogContent className="max-w-sm">
            <DialogHeader>
              <DialogTitle>Change Student?</DialogTitle>
            </DialogHeader>
            <div className="text-sm text-muted-foreground mb-4">
              You have uploaded signature images for the current student. Changing the student will automatically clear all uploaded images.
            </div>
            <div className="flex gap-2 justify-end">
              <Button variant="outline" onClick={cancelStudentSwitch}>
                Cancel
              </Button>
              <Button variant="destructive" onClick={confirmStudentSwitch}>
                Change Student
              </Button>
            </div>
          </DialogContent>
        </Dialog>

        {/* Unsaved Changes Dialog */}
        <UnsavedChangesDialog
          open={showConfirmDialog}
          onConfirm={() => {
            confirmClose();
            // Clear any pending navigation
            setPendingNavigation(null);
          }}
          onCancel={() => {
            cancelClose();
            // Clear any pending navigation
            setPendingNavigation(null);
          }}
        />
      </div>
    </Layout>
  );
};

export default SignatureAI;



training.py
from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from typing import List, Optional
import numpy as np
from PIL import Image
import io
import os
import uuid
from datetime import datetime
import time
import logging
import asyncio

from models.database import db_manager
from models.signature_model import SignatureVerificationModel
from utils.image_processing import preprocess_image
from utils.storage import save_to_supabase, cleanup_local_file
from utils.augmentation import SignatureAugmentation
from utils.job_queue import job_queue
from services.model_versioning import model_versioning_service
from config import settings

router = APIRouter()
logger = logging.getLogger(__name__)

# Global model instance
model_manager = SignatureVerificationModel()

@router.post("/start")
async def start_training(
    student_id: str = Form(...),
    genuine_files: List[UploadFile] = File(...),
    forged_files: List[UploadFile] = File(...)
):
    """Start training a signature verification model for a student"""
    
    try:
        # Validate student exists by school/student_id string; fallback to numeric id
        student = await db_manager.get_student_by_school_id(student_id)
        if not student:
            try:
                numeric_id = int(student_id)
                student = await db_manager.get_student(numeric_id)
            except Exception:
                student = None
        if not student:
            raise HTTPException(status_code=404, detail="Student not found")
        
        # Validate minimum samples
        if len(genuine_files) < settings.MIN_GENUINE_SAMPLES:
            raise HTTPException(
                status_code=400, 
                detail=f"Minimum {settings.MIN_GENUINE_SAMPLES} genuine samples required"
            )
        
        if len(forged_files) < settings.MIN_FORGED_SAMPLES:
            raise HTTPException(
                status_code=400, 
                detail=f"Minimum {settings.MIN_FORGED_SAMPLES} forged samples required"
            )
        
        # Process and validate images
        genuine_images = []
        forged_images = []
        
        # Process genuine images
        for file in genuine_files:
            
            image_data = await file.read()
            image = Image.open(io.BytesIO(image_data))
            processed_image = preprocess_image(image, settings.MODEL_IMAGE_SIZE)
            genuine_images.append(processed_image)
        
        # Process forged images
        for file in forged_files:
            
            image_data = await file.read()
            image = Image.open(io.BytesIO(image_data))
            processed_image = preprocess_image(image, settings.MODEL_IMAGE_SIZE)
            forged_images.append(processed_image)
        
        # Apply data augmentation to increase training robustness
        logger.info(f"Applying data augmentation to {len(genuine_images)} genuine and {len(forged_images)} forged samples")
        augmenter = SignatureAugmentation(
            rotation_range=15.0,
            scale_range=(0.9, 1.1),
            brightness_range=0.2,
            blur_probability=0.3,
            thickness_variation=0.1
        )
        
        # Augment genuine signatures more aggressively (3x augmentation)
        genuine_augmented, genuine_labels = augmenter.augment_batch(
            genuine_images, [True] * len(genuine_images), augmentation_factor=3
        )
        
        # Augment forged signatures less aggressively (1x augmentation)
        forged_augmented, forged_labels = augmenter.augment_batch(
            forged_images, [False] * len(forged_images), augmentation_factor=1
        )
        
        # Combine all augmented data
        all_images = genuine_augmented + forged_augmented
        all_labels = genuine_labels + forged_labels
        
        logger.info(f"After augmentation: {len(all_images)} total samples ({len(genuine_augmented)} genuine, {len(forged_augmented)} forged)")
        
        # Create model record in database
        model_uuid = str(uuid.uuid4())
        model_data = {
            # Let DB auto-generate bigint primary key id
            "student_id": int(student["id"]) if isinstance(student.get("id"), (int, float)) else student.get("id"),
            "model_path": f"models/{model_uuid}.keras",
            "status": "training",
            "sample_count": len(genuine_images) + len(forged_images),
            "genuine_count": len(genuine_images),
            "forged_count": len(forged_images),
            "training_date": datetime.utcnow().isoformat()
        }
        created = await db_manager.create_trained_model(model_data)
        numeric_model_id = created["id"] if isinstance(created, dict) else None
        
        # Start training (this would be async in production)
        try:
            t0 = time.time()
            # Train with augmented data for better robustness
            history = model_manager.train_with_augmented_data(all_images, all_labels)

            # Compute prototype (centroid) from genuine samples
            centroid, _ = model_manager.compute_centroid_and_threshold(genuine_images)
            # Calibrate threshold via EER using distances to centroid
            gen_emb = model_manager.embed_images(genuine_images)
            forg_emb = model_manager.embed_images(forged_images) if len(forged_images) else np.zeros((0, gen_emb.shape[1]))
            centroid_vec = np.array(centroid)
            gen_d = np.linalg.norm(gen_emb - centroid_vec, axis=1)
            forg_d = np.linalg.norm(forg_emb - centroid_vec, axis=1) if forg_emb.size else np.array([])

            def compute_eer_threshold(genuine_d, forged_d):
                if len(genuine_d) == 0:
                    return 0.7, 0.0, 0.0
                # Candidate thresholds are unique distances observed
                all_d = np.concatenate([genuine_d, forged_d]) if len(forged_d) else genuine_d
                all_d = np.unique(np.sort(all_d))
                best_thr = all_d[0] if all_d.size > 0 else 0.7
                best_gap = 1e9
                best_far = 0.0
                best_frr = 0.0
                total_gen = max(1, len(genuine_d))
                total_forg = max(1, len(forged_d))
                for thr in all_d:
                    # Accept if distance <= thr
                    frr = float(np.sum(genuine_d > thr)) / total_gen
                    far = float(np.sum(forged_d <= thr)) / total_forg if total_forg > 0 else 0.0
                    gap = abs(far - frr)
                    if gap < best_gap:
                        best_gap = gap
                        best_thr = thr
                        best_far = far
                        best_frr = frr
                return float(best_thr), float(best_far), float(best_frr)

            threshold, far_at_thr, frr_at_thr = compute_eer_threshold(gen_d, forg_d)
            
            # Get best accuracy
            best_accuracy = max(history.history['val_accuracy'])
            train_time_s = int(time.time() - t0)
            best_precision = max(history.history.get('precision', [0])) if history.history.get('precision') else 0
            best_recall = max(history.history.get('recall', [0])) if history.history.get('recall') else 0
            f1 = (2 * best_precision * best_recall / (best_precision + best_recall)) if (best_precision + best_recall) > 0 else 0
            
            # Save full model (Keras format) and embedding-only model
            local_model_path = os.path.join(settings.LOCAL_MODELS_DIR, f"{model_uuid}.keras")
            model_manager.save_model(local_model_path)

            # Save embedding-only model if available
            embedding_local_path = None
            if getattr(model_manager, 'embedding_model', None) is not None:
                embedding_local_path = os.path.join(settings.LOCAL_MODELS_DIR, f"{model_uuid}_embed.keras")
                model_manager.embedding_model.save(embedding_local_path)
            
            # Upload to Supabase Storage
            supabase_path = await save_to_supabase(local_model_path, f"models/{model_uuid}.keras")
            embedding_supabase_path = None
            if embedding_local_path:
                embedding_supabase_path = await save_to_supabase(embedding_local_path, f"models/{model_uuid}_embed.keras")
            
            # Update model record
            await db_manager.update_model_status(
                numeric_model_id,
                "completed",
                float(best_accuracy)
            )
            # Store prototype metadata and final artifact paths
            await db_manager.update_model_metadata(numeric_model_id, {
                "model_path": supabase_path,
                "prototype_centroid": centroid,
                "prototype_threshold": threshold,
                "embedding_model_path": embedding_supabase_path
            })
            
            # Cleanup local file
            cleanup_local_file(local_model_path)
            if embedding_local_path:
                cleanup_local_file(embedding_local_path)
            
            return {
                "success": True,
                "model_id": numeric_model_id,
                "model_uuid": model_uuid,
                "status": "completed",
                "accuracy": float(best_accuracy),
                "val_accuracy": float(best_accuracy),
                "precision": float(best_precision),
                "recall": float(best_recall),
                "f1": float(f1),
                "training_samples": len(genuine_images) + len(forged_images),
                "genuine_count": len(genuine_images),
                "forged_count": len(forged_images),
                "train_time_s": train_time_s,
                "calibration": {
                    "threshold": float(threshold),
                    "far": float(far_at_thr),
                    "frr": float(frr_at_thr)
                },
                "message": "Model trained successfully"
            }
            
        except Exception as e:
            # Update model status to failed
            try:
                if numeric_model_id is not None:
                    await db_manager.update_model_status(numeric_model_id, "failed")
            except Exception:
                pass
            logger.error(f"Training failed: {e}")
            raise HTTPException(status_code=500, detail=f"Training failed: {str(e)}")
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error in training: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@router.get("/status/{model_id}")
async def get_training_status(model_id: str):
    """Get the training status of a model"""
    try:
        models = await db_manager.get_trained_models()
        model = next((m for m in models if m["id"] == model_id), None)
        
        if not model:
            raise HTTPException(status_code=404, detail="Model not found")
        
        return {
            "model_id": model_id,
            "status": model["status"],
            "accuracy": model.get("accuracy"),
            "training_date": model["training_date"],
            "sample_count": model["sample_count"]
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting training status: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.post("/start-async")
async def start_async_training(
    student_id: str = Form(...),
    genuine_files: List[UploadFile] = File(...),
    forged_files: List[UploadFile] = File(...)
):
    """Start async training job for a signature verification model"""
    
    try:
        # Validate student exists
        student = await db_manager.get_student_by_school_id(student_id)
        if not student:
            try:
                numeric_id = int(student_id)
                student = await db_manager.get_student(numeric_id)
            except Exception:
                student = None
        if not student:
            raise HTTPException(status_code=404, detail="Student not found")
        
        # Validate minimum samples
        if len(genuine_files) < settings.MIN_GENUINE_SAMPLES:
            raise HTTPException(
                status_code=400, 
                detail=f"Minimum {settings.MIN_GENUINE_SAMPLES} genuine samples required"
            )
        
        if len(forged_files) < settings.MIN_FORGED_SAMPLES:
            raise HTTPException(
                status_code=400, 
                detail=f"Minimum {settings.MIN_FORGED_SAMPLES} forged samples required"
            )
        
        # Create async training job
        job = job_queue.create_job(int(student["id"]), "training")
        
        # Start training in background
        # Read and store file contents before spawning background task
        genuine_data = [await f.read() for f in genuine_files]
        forged_data = [await f.read() for f in forged_files]

        asyncio.create_task(run_async_training(job, student, genuine_data, forged_data))

        
        return {
            "success": True,
            "job_id": job.job_id,
            "message": "Training job started",
            "stream_url": f"/api/progress/stream/{job.job_id}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting async training: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

async def run_async_training(job, student, genuine_files, forged_files):
    """Run training job in background with progress updates."""
    try:
        job_queue.start_job(job.job_id)
        job_queue.update_job_progress(job.job_id, 5.0, "Processing images...")
        
        # Process and validate images
        genuine_images = []
        forged_images = []
        
        # Process genuine images
        for i, data in enumerate(genuine_files):
            
           
            image = Image.open(io.BytesIO(data))
            processed_image = preprocess_image(image, settings.MODEL_IMAGE_SIZE)
            genuine_images.append(processed_image)
            
            # Update progress
            progress = 5.0 + (i + 1) / len(genuine_files) * 15.0
            job_queue.update_job_progress(job.job_id, progress, f"Processing genuine images... {i+1}/{len(genuine_files)}")
        
        # Process forged images
        for i, data in enumerate(forged_files):
            
           
            image = Image.open(io.BytesIO(data))
            processed_image = preprocess_image(image, settings.MODEL_IMAGE_SIZE)
            forged_images.append(processed_image)
            
            # Update progress
            progress = 20.0 + (i + 1) / len(forged_files) * 15.0
            job_queue.update_job_progress(job.job_id, progress, f"Processing forged images... {i+1}/{len(forged_files)}")
        
        # Apply data augmentation
        job_queue.update_job_progress(job.job_id, 35.0, "Applying data augmentation...")
        augmenter = SignatureAugmentation(
            rotation_range=15.0,
            scale_range=(0.9, 1.1),
            brightness_range=0.2,
            blur_probability=0.3,
            thickness_variation=0.1
        )
        
        genuine_augmented, genuine_labels = augmenter.augment_batch(
            genuine_images, [True] * len(genuine_images), augmentation_factor=3
        )
        
        forged_augmented, forged_labels = augmenter.augment_batch(
            forged_images, [False] * len(forged_images), augmentation_factor=1
        )
        
        all_images = genuine_augmented + forged_augmented
        all_labels = genuine_labels + forged_labels
        
        job_queue.update_job_progress(job.job_id, 40.0, "Creating model record...")
        
        # Create model record in database
        model_uuid = str(uuid.uuid4())
        model_data = {
            "student_id": int(student["id"]),
            "model_path": f"models/{model_uuid}.keras",
            "status": "training",
            "sample_count": len(genuine_images) + len(forged_images),
            "genuine_count": len(genuine_images),
            "forged_count": len(forged_images),
            "training_date": datetime.utcnow().isoformat()
        }
        created = await db_manager.create_trained_model(model_data)
        numeric_model_id = created["id"] if isinstance(created, dict) else None
        
        # Start training
        job_queue.update_job_progress(job.job_id, 45.0, "Initializing AI model...")
        model_manager = SignatureVerificationModel()
        
        job_queue.update_job_progress(job.job_id, 50.0, "Training AI model...")
        t0 = time.time()
        history = model_manager.train_with_augmented_data(all_images, all_labels)
        
        # Compute prototype and threshold
        job_queue.update_job_progress(job.job_id, 80.0, "Computing prototype and threshold...")
        centroid, _ = model_manager.compute_centroid_and_threshold(genuine_images)
        
        # Calibrate threshold via EER
        gen_emb = model_manager.embed_images(genuine_images)
        forg_emb = model_manager.embed_images(forged_images) if len(forged_images) else np.zeros((0, gen_emb.shape[1]))
        centroid_vec = np.array(centroid)
        gen_d = np.linalg.norm(gen_emb - centroid_vec, axis=1)
        forg_d = np.linalg.norm(forg_emb - centroid_vec, axis=1) if forg_emb.size else np.array([])
        
        def compute_eer_threshold(genuine_d, forged_d):
            if len(genuine_d) == 0:
                return 0.7, 0.0, 0.0
            all_d = np.concatenate([genuine_d, forged_d]) if len(forged_d) else genuine_d
            all_d = np.unique(np.sort(all_d))
            best_thr = all_d[0] if all_d.size > 0 else 0.7
            best_gap = 1e9
            best_far = 0.0
            best_frr = 0.0
            
            for thr in all_d:
                far = np.sum(forged_d <= thr) / len(forged_d) if len(forged_d) > 0 else 0.0
                frr = np.sum(genuine_d > thr) / len(genuine_d)
                gap = abs(far - frr)
                if gap < best_gap:
                    best_gap = gap
                    best_thr = thr
                    best_far = far
                    best_frr = frr
            
            return best_thr, best_far, best_frr
        
        threshold, far, frr = compute_eer_threshold(gen_d, forg_d)
        
        # Save models
        job_queue.update_job_progress(job.job_id, 85.0, "Saving model...")
        local_model_path = os.path.join(settings.LOCAL_MODELS_DIR, f"{model_uuid}.keras")
        local_embed_path = os.path.join(settings.LOCAL_MODELS_DIR, f"{model_uuid}_embed.keras")
        
        model_manager.save_model(local_model_path)
        model_manager.save_embedding_model(local_embed_path)
        
        # Upload to Supabase
        job_queue.update_job_progress(job.job_id, 90.0, "Uploading to cloud storage...")
        remote_model_path = await save_to_supabase(local_model_path, f"models/{model_uuid}.keras")
        remote_embed_path = await save_to_supabase(local_embed_path, f"models/{model_uuid}_embed.keras")
        
        # Update model metadata
        if numeric_model_id:
            await db_manager.update_model_metadata(numeric_model_id, {
                "status": "completed",
                "accuracy": float(history.history.get("val_accuracy", [0])[-1]),
                "prototype_centroid": centroid.tolist(),
                "prototype_threshold": float(threshold),
                "embedding_model_path": remote_embed_path,
                "far": float(far),
                "frr": float(frr)
            })
        
        # Cleanup local files
        cleanup_local_file(local_model_path)
        cleanup_local_file(local_embed_path)
        
        # Calculate training metrics
        train_time = time.time() - t0
        val_accuracy = history.history.get("val_accuracy", [0])[-1]
        precision = history.history.get("val_precision", [0])[-1]
        recall = history.history.get("val_recall", [0])[-1]
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        result = {
            "success": True,
            "accuracy": float(history.history.get("accuracy", [0])[-1]),
            "val_accuracy": float(val_accuracy),
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "train_time_s": float(train_time),
            "threshold": float(threshold),
            "far": float(far),
            "frr": float(frr),
            "model_id": numeric_model_id
        }
        
        # Create model version for tracking
        if numeric_model_id:
            try:
                performance_metrics = {
                    "accuracy": result["accuracy"],
                    "val_accuracy": result["val_accuracy"],
                    "precision": result["precision"],
                    "recall": result["recall"],
                    "f1": result["f1"],
                    "far": result["far"],
                    "frr": result["frr"],
                    "threshold": result["threshold"],
                    "train_time_s": result["train_time_s"]
                }
                
                await model_versioning_service.create_model_version(
                    model_id=numeric_model_id,
                    version_notes=f"Auto-created version after training completion",
                    created_by="system",
                    performance_metrics=performance_metrics
                )
                logger.info(f"Created model version for model {numeric_model_id}")
            except Exception as e:
                logger.error(f"Error creating model version: {e}")
                # Don't fail the job for versioning errors
        
        job_queue.complete_job(job.job_id, result)
        
    except Exception as e:
        logger.error(f"Async training failed: {e}")
        job_queue.fail_job(job.job_id, str(e))

@router.get("/models")
async def get_trained_models(student_id: Optional[int] = None):
    """Get all trained models, optionally filtered by student"""
    try:
        models = await db_manager.get_trained_models(student_id)
        return {"models": models}
    
    except Exception as e:
        logger.error(f"Error getting trained models: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


verification.py
from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from typing import List
import numpy as np
from PIL import Image
import io
import logging
import time

from models.database import db_manager
import tensorflow as tf
from tensorflow import keras
from models.signature_model import SignatureVerificationModel
from utils.image_processing import preprocess_image
from utils.storage import download_from_supabase
from utils.antispoofing import AntiSpoofingDetector
from services.model_versioning import model_versioning_service
from config import settings

router = APIRouter()
logger = logging.getLogger(__name__)

@router.post("/verify")
async def verify_signature(
    student_id: str = Form(...),
    test_file: UploadFile = File(...)
):
    """Verify a signature for a student using stored prototype (no references required)"""
    
    try:
        # Resolve student (accept school_id or numeric id)
        student = await db_manager.get_student_by_school_id(student_id)
        if not student:
            try:
                numeric_id = int(student_id)
                student = await db_manager.get_student(numeric_id)
            except Exception:
                student = None
        if not student:
            raise HTTPException(status_code=404, detail="Student not found")

        # Get trained model for this student
        models = await db_manager.get_trained_models(int(student["id"]))
        # Prefer newest completed model that has prototype metadata
        candidates = [m for m in models if m.get("status") == "completed" and m.get("prototype_centroid") is not None]
        if not candidates:
            # Fallback: any completed model
            candidates = [m for m in models if m.get("status") == "completed"]
        # Sort by created_at/updated_at desc if present
        def ts(m):
            return m.get("updated_at") or m.get("created_at") or ""
        candidates = sorted(candidates, key=ts, reverse=True)
        model = candidates[0] if candidates else None
        
        if not model:
            raise HTTPException(status_code=404, detail="Model not found for student")

        
        # Process test image
        test_data = await test_file.read()
        test_image = Image.open(io.BytesIO(test_data))
        test_processed = preprocess_image(test_image, settings.MODEL_IMAGE_SIZE)
        
        # Record start time for performance tracking
        start_time = time.time()
        
        # Perform anti-spoofing analysis
        spoofing_detector = AntiSpoofingDetector()
        spoofing_analysis = spoofing_detector.analyze_signature(test_processed)
        
        # Log spoofing analysis for monitoring
        logger.info(f"Anti-spoofing analysis for student {student_id}: {spoofing_analysis}")
        
        # Load the trained model for embedding
        model_manager = SignatureVerificationModel()
        # Prefer embedding-only artifact if present (no Lambda)
        embed_path_remote = model.get("embedding_model_path")
        if embed_path_remote:
            embed_local = await download_from_supabase(embed_path_remote)
            model_manager.embedding_model = keras.models.load_model(embed_local, safe_mode=False)
        else:
            model_path = await download_from_supabase(model["model_path"])
            try:
                model_manager.load_model(model_path)
            except Exception as e:
                raise HTTPException(status_code=400, detail="Model artifact is from an old version. Please retrain this student and try again.")

        # Compute embedding for test image and compare to stored prototype
        test_arr = np.array(test_processed)
        test_tensor = tf.convert_to_tensor(test_arr)
        test_tensor = model_manager.preprocess_image(test_tensor)
        test_tensor = tf.expand_dims(test_tensor, axis=0)
        test_embedding = model_manager.embedding_model.predict(test_tensor, verbose=0)[0]

        centroid = np.array(model.get("prototype_centroid") or [])
        # Default threshold fallback
        try:
            threshold = float(model.get("prototype_threshold")) if model.get("prototype_threshold") is not None else 0.7
        except Exception:
            threshold = 0.7
        if centroid.size == 0:
            raise HTTPException(status_code=400, detail="Prototype not available for this model")

        dist = float(np.linalg.norm(test_embedding - centroid))
        is_genuine = dist <= threshold
        denom = threshold if threshold and threshold > 1e-6 else 1.0
        raw_score = 1.0 - dist / denom
        score = float(max(0.0, min(1.0, raw_score)))

        # Generate spoofing warning message
        spoofing_warning = spoofing_detector.get_spoofing_warning_message(spoofing_analysis)
        
        # Prepare verification result
        verification_result = {
            "success": True,
            "student_id": student_id,
            "match": is_genuine,
            "distance": dist,
            "threshold": threshold,
            "score": score,
            "message": "Verified against prototype",
            "antispoofing": {
                "is_potentially_spoofed": spoofing_analysis.get("is_potentially_spoofed", False),
                "is_likely_printed": spoofing_analysis.get("is_likely_printed", False),
                "is_low_quality": spoofing_analysis.get("is_low_quality", False),
                "printed_confidence": spoofing_analysis.get("printed_confidence", 0.0),
                "quality_score": spoofing_analysis.get("quality_score", 0.0),
                "warning_message": spoofing_warning
            }
        }
        
        # Record verification result for A/B testing and analytics
        try:
            processing_time = int((time.time() - start_time) * 1000)  # Convert to milliseconds
            await model_versioning_service.record_verification_result(
                student_id=int(student["id"]),
                model_id=model["id"],
                verification_result=verification_result,
                processing_time_ms=processing_time
            )
        except Exception as e:
            logger.error(f"Error recording verification result: {e}")
            # Don't fail verification for analytics errors
        
        return verification_result
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in signature verification: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/models/{student_id}")
async def get_verification_models(student_id: int):
    """Get available models for verification for a specific student"""
    try:
        models = await db_manager.get_trained_models(student_id)
        completed_models = [m for m in models if m["status"] == "completed"]
        
        return {
            "student_id": student_id,
            "available_models": completed_models
        }
    
    except Exception as e:
        logger.error(f"Error getting verification models: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


signature_model.py
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
from config import settings
import logging

logger = logging.getLogger(__name__)

class SignatureVerificationModel:
    def __init__(self):
        self.model = None
        self.embedding_model = None
        self.image_size = settings.MODEL_IMAGE_SIZE
        self.batch_size = settings.MODEL_BATCH_SIZE
        self.epochs = settings.MODEL_EPOCHS
        self.learning_rate = settings.MODEL_LEARNING_RATE
    
    def create_siamese_network(self):
        """Create a Siamese neural network for signature verification"""
        
        # Input layers
        input_a = layers.Input(shape=(self.image_size, self.image_size, 3), name='input_a')
        input_b = layers.Input(shape=(self.image_size, self.image_size, 3), name='input_b')
        
        # Shared CNN backbone - optimized for CPU
        def create_embedding_branch():
            model = keras.Sequential([
                # First block - reduced filters for CPU efficiency
                layers.Conv2D(24, (5, 5), activation='relu', padding='same',
                            input_shape=(self.image_size, self.image_size, 3)),
                layers.BatchNormalization(),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                # Second block
                layers.Conv2D(48, (3, 3), activation='relu', padding='same'),
                layers.BatchNormalization(),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.25),
                
                # Third block
                layers.Conv2D(96, (3, 3), activation='relu', padding='same'),
                layers.BatchNormalization(),
                layers.MaxPooling2D((2, 2)),
                layers.Dropout(0.3),
                
                # Fourth block - signature-specific features
                layers.Conv2D(192, (3, 3), activation='relu', padding='same'),
                layers.BatchNormalization(),
                layers.GlobalAveragePooling2D(),
                
                # Dense layers for feature extraction
                layers.Dense(256, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.4),
                layers.Dense(128, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                layers.Dense(64, activation=None)  # No activation for embeddings
            ], name='embedding_branch')
            return model
        
        # Create shared embedding network
        embedding_network = create_embedding_branch()
        
        # Get embeddings for both inputs
        embedding_a = embedding_network(input_a)
        embedding_b = embedding_network(input_b)
        
        # Compute L2 distance between embeddings (no Lambda layer)
        # Using Subtract + custom layer for compatibility
        diff = layers.Subtract(name='embedding_diff')([embedding_a, embedding_b])
        
        # Custom layer to compute absolute difference without Lambda
        class AbsDiffLayer(layers.Layer):
            def call(self, inputs):
                return tf.abs(inputs)
        
        abs_diff = AbsDiffLayer(name='abs_diff')(diff)
        
        # Classification head with residual connections
        x = layers.Dense(32, activation='relu')(abs_diff)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        # Add skip connection for better gradient flow
        x2 = layers.Dense(16, activation='relu')(x)
        x2 = layers.BatchNormalization()(x2)
        
        # Output layer
        output = layers.Dense(1, activation='sigmoid', name='similarity_score')(x2)
        
        # Create the model
        model = keras.Model(inputs=[input_a, input_b], outputs=output, name='signature_verification')
        
        # Use AdamW optimizer for better generalization
        optimizer = keras.optimizers.AdamW(
            learning_rate=self.learning_rate,
            weight_decay=0.0001
        )
        
        # Compile with additional metrics
        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
                keras.metrics.AUC(name='auc')
            ]
        )
        
        # Create standalone embedding model for inference
        single_input = layers.Input(shape=(self.image_size, self.image_size, 3), name='single_input')
        single_embedding = embedding_network(single_input)
        self.embedding_model = keras.Model(inputs=single_input, outputs=single_embedding, name='embedding_model')

        return model
    
    def prepare_augmented_data(self, all_images, all_labels):
        """Prepare balanced training data from augmented images"""
        images = np.array(all_images)
        labels = np.array(all_labels)
        
        pairs = []
        pair_labels = []
        
        # Get indices
        genuine_indices = np.where(labels == True)[0]
        forged_indices = np.where(labels == False)[0]
        
        # Balance the dataset
        num_genuine = len(genuine_indices)
        num_forged = len(forged_indices)
        
        # Create positive pairs (same class)
        # Genuine-Genuine pairs
        for i in range(num_genuine):
            # Create 2-3 pairs per genuine sample
            for j in range(i + 1, min(i + 3, num_genuine)):
                idx1, idx2 = genuine_indices[i], genuine_indices[j]
                pairs.append([images[idx1], images[idx2]])
                pair_labels.append(1)  # Similar
        
        # Create negative pairs (different classes)
        # Genuine-Forged pairs - crucial for discrimination
        max_neg_pairs = len(pairs) * 2  # 2:1 negative to positive ratio
        neg_pair_count = 0
        
        for i in range(num_genuine):
            for j in range(min(3, num_forged)):  # 3 forged samples per genuine
                if neg_pair_count >= max_neg_pairs:
                    break
                genuine_idx = genuine_indices[i]
                forged_idx = forged_indices[j % num_forged]
                pairs.append([images[genuine_idx], images[forged_idx]])
                pair_labels.append(0)  # Different
                neg_pair_count += 1
        
        # Convert to arrays and shuffle
        pairs = np.array(pairs)
        pair_labels = np.array(pair_labels)
        
        # Shuffle with fixed seed for reproducibility
        indices = np.arange(len(pairs))
        np.random.RandomState(42).shuffle(indices)
        pairs = pairs[indices]
        pair_labels = pair_labels[indices]
        
        # Split into input arrays
        input_a = pairs[:, 0]
        input_b = pairs[:, 1]
        
        logger.info(f"Created {len(pairs)} training pairs: {np.sum(pair_labels)} positive, {len(pairs) - np.sum(pair_labels)} negative")
        
        return input_a, input_b, pair_labels
    
    def train_with_augmented_data(self, all_images, all_labels, validation_split=0.2):
        """Train the signature verification model with augmented data"""
        try:
            # Prepare balanced data
            input_a, input_b, labels = self.prepare_augmented_data(all_images, all_labels)
            
            # Create model
            self.model = self.create_siamese_network()
            
            # Callbacks for CPU-optimized training
            callbacks = [
                keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=15,
                    restore_best_weights=True,
                    verbose=1
                ),
                keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=7,
                    min_lr=1e-6,
                    verbose=1
                ),
                keras.callbacks.ModelCheckpoint(
                    filepath='temp_best_model.keras',
                    monitor='val_auc',
                    mode='max',
                    save_best_only=True,
                    verbose=0
                )
            ]
            
            # Custom training configuration for CPU
            # Reduce batch size for CPU memory efficiency
            cpu_batch_size = min(self.batch_size, 16)
            
            # Train the model
            history = self.model.fit(
                [input_a, input_b],
                labels,
                batch_size=cpu_batch_size,
                epochs=self.epochs,
                validation_split=validation_split,
                callbacks=callbacks,
                verbose=1,
                workers=2,  # CPU workers
                use_multiprocessing=False  # Safer for CPU
            )
            
            # Load best model if checkpoint exists
            try:
                self.model = keras.models.load_model('temp_best_model.keras', safe_mode=False)
                import os
                if os.path.exists('temp_best_model.keras'):
                    os.remove('temp_best_model.keras')
            except:
                pass
            
            return history
            
        except Exception as e:
            logger.error(f"Error during training: {e}")
            raise
    
    def compute_centroid_and_adaptive_threshold(self, genuine_images, forged_images=None):
        """Compute centroid and adaptive threshold using statistical methods"""
        if self.embedding_model is None:
            raise ValueError("Embedding model not available")
        
        # Get genuine embeddings
        genuine_embeddings = self.embed_images(genuine_images)
        
        # Compute centroid (prototype)
        centroid = np.mean(genuine_embeddings, axis=0)
        
        # Calculate distances from genuine samples to centroid
        genuine_distances = np.linalg.norm(genuine_embeddings - centroid, axis=1)
        
        # Statistical threshold calculation
        mean_dist = np.mean(genuine_distances)
        std_dist = np.std(genuine_distances)
        
        # Adaptive threshold strategies
        if forged_images is not None and len(forged_images) > 0:
            # If we have forged samples, use them for better calibration
            forged_embeddings = self.embed_images(forged_images)
            forged_distances = np.linalg.norm(forged_embeddings - centroid, axis=1)
            
            # Find optimal threshold using ROC curve analysis
            threshold = self._find_optimal_threshold(genuine_distances, forged_distances)
        else:
            # Conservative threshold: mean + 2*std covers ~95% of genuine signatures
            threshold = mean_dist + 2.0 * std_dist
            
            # Apply bounds to prevent extreme thresholds
            max_threshold = mean_dist + 3.0 * std_dist
            min_threshold = mean_dist + 0.5 * std_dist
            threshold = np.clip(threshold, min_threshold, max_threshold)
        
        logger.info(f"Threshold calibration: mean={mean_dist:.3f}, std={std_dist:.3f}, threshold={threshold:.3f}")
        
        return centroid.tolist(), float(threshold)
    
    def _find_optimal_threshold(self, genuine_distances, forged_distances):
        """Find optimal threshold using Equal Error Rate (EER)"""
        all_distances = np.concatenate([genuine_distances, forged_distances])
        thresholds = np.percentile(all_distances, np.linspace(5, 95, 50))
        
        best_threshold = thresholds[0]
        best_score = float('inf')
        
        for threshold in thresholds:
            # False Rejection Rate (genuine rejected)
            frr = np.sum(genuine_distances > threshold) / len(genuine_distances)
            # False Acceptance Rate (forged accepted)
            far = np.sum(forged_distances <= threshold) / len(forged_distances)
            
            # Find threshold where FRR ≈ FAR (Equal Error Rate)
            eer_score = abs(frr - far)
            
            # Also consider overall error
            total_error = 0.5 * (frr + far)
            combined_score = eer_score + 0.1 * total_error
            
            if combined_score < best_score:
                best_score = combined_score
                best_threshold = threshold
        
        return float(best_threshold)
    
    def embed_images(self, images):
        """Generate embeddings for a list of PIL images"""
        if self.embedding_model is None:
            raise ValueError("Embedding model not available")
        
        batch = []
        for img in images:
            # Handle both PIL and numpy arrays
            if hasattr(img, 'convert'):  # PIL Image
                arr = np.array(img.convert('RGB'))
            else:
                arr = np.array(img)
            
            # Ensure correct shape
            if arr.shape[-1] != 3:
                if len(arr.shape) == 2:  # Grayscale
                    arr = np.stack([arr] * 3, axis=-1)
                elif arr.shape[-1] == 4:  # RGBA
                    arr = arr[:, :, :3]
            
            arr = tf.convert_to_tensor(arr, dtype=tf.float32)
            arr = self.preprocess_image(arr)
            batch.append(arr)
        
        batch_tensor = tf.stack(batch, axis=0)
        embeddings = self.embedding_model.predict(batch_tensor, verbose=0)
        return embeddings
    
    def preprocess_image(self, image):
        """Enhanced preprocessing for better feature extraction"""
        # Resize image
        image = tf.image.resize(image, [self.image_size, self.image_size])
        
        # Normalize to [-1, 1] for better gradient flow
        image = tf.cast(image, tf.float32) / 127.5 - 1.0
        
        return image
    
    def save_embedding_model(self, filepath):
        """Save the embedding model separately"""
        if self.embedding_model is None:
            raise ValueError("No embedding model to save")
        self.embedding_model.save(filepath)
        logger.info(f"Embedding model saved to {filepath}")
    
    def save_model(self, filepath):
        """Save the trained model"""
        if self.model is None:
            raise ValueError("No model to save")
        self.model.save(filepath)
        logger.info(f"Model saved to {filepath}")
    
    def load_model(self, filepath):
        """Load a trained model"""
        self.model = keras.models.load_model(filepath, safe_mode=False)
        
        # Reconstruct embedding model
        try:
            embedding_branch = self.model.get_layer('embedding_branch')
            single_input = layers.Input(shape=(self.image_size, self.image_size, 3))
            single_embedding = embedding_branch(single_input)
            self.embedding_model = keras.Model(inputs=single_input, outputs=single_embedding)
        except Exception as e:
            logger.error(f"Could not reconstruct embedding model: {e}")
            self.embedding_model = None
        
        logger.info(f"Model loaded from {filepath}")


augmentation.py
import numpy as np
import numpy as np
from PIL import Image, ImageFilter, ImageEnhance, ImageOps
import cv2
import random
import logging

logger = logging.getLogger(__name__)

class SignatureAugmentation:
    """Enhanced data augmentation specifically for signature images"""
    
    def __init__(self,
                 rotation_range=15.0,
                 scale_range=(0.9, 1.1),
                 brightness_range=0.2,
                 blur_probability=0.3,
                 thickness_variation=0.15,
                 noise_probability=0.2,
                 shear_range=0.1):
        
        self.rotation_range = rotation_range
        self.scale_range = scale_range
        self.brightness_range = brightness_range
        self.blur_probability = blur_probability
        self.thickness_variation = thickness_variation
        self.noise_probability = noise_probability
        self.shear_range = shear_range
    
    def augment_single(self, image, is_genuine=True):
        """Apply augmentation to a single image"""
        # Convert to PIL if numpy array
        if isinstance(image, np.ndarray):
            image = Image.fromarray((image * 255).astype(np.uint8) if image.max() <= 1 else image.astype(np.uint8))
        
        # Ensure RGB
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Apply augmentations with different probabilities for genuine vs forged
        augment_prob = 0.8 if is_genuine else 0.6
        
        if random.random() < augment_prob:
            # Rotation
            if random.random() < 0.7:
                angle = random.uniform(-self.rotation_range, self.rotation_range)
                image = image.rotate(angle, fillcolor=(255, 255, 255), expand=False)
            
            # Scaling
            if random.random() < 0.6:
                scale = random.uniform(*self.scale_range)
                new_size = (int(image.width * scale), int(image.height * scale))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                
                # Center crop or pad to original size
                if scale > 1:
                    # Crop
                    original_width = int(image.width / scale)
                    original_height = int(image.height / scale)
                    left = (image.width - original_width) // 2
                    top = (image.height - original_height) // 2
                    right = left + original_width
                    bottom = top + original_height
                    image = image.crop((left, top, right, bottom))
                else:
                    # Pad
                    original_width = int(image.width / scale)
                    original_height = int(image.height / scale)
                    new_image = Image.new('RGB', (original_width, original_height), (255, 255, 255))
                    paste_x = (new_image.width - image.width) // 2
                    paste_y = (new_image.height - image.height) // 2
                    new_image.paste(image, (paste_x, paste_y))
                    image = new_image
            
            # Brightness variation
            if random.random() < 0.5:
                enhancer = ImageEnhance.Brightness(image)
                factor = 1.0 + random.uniform(-self.brightness_range, self.brightness_range)
                image = enhancer.enhance(factor)
            
            # Blur (simulates pen pressure variation)
            if random.random() < self.blur_probability:
                blur_type = random.choice(['gaussian', 'motion'])
                if blur_type == 'gaussian':
                    image = image.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 1.5)))
                else:
                    # Motion blur for pen movement simulation
                    image = self._apply_motion_blur(image)
            
            # Thickness variation (morphological operations)
            if random.random() < 0.5:
                image = self._vary_stroke_thickness(image, self.thickness_variation)
            
            # Add noise
            if random.random() < self.noise_probability:
                image = self._add_noise(image)
            
            # Shear transformation
            if random.random() < 0.4:
                image = self._apply_shear(image, self.shear_range)
        
        return image
    
    def _apply_motion_blur(self, image):
        """Apply motion blur to simulate pen movement"""
        # Convert to numpy
        img_array = np.array(image)
        
        # Random kernel size and angle
        kernel_size = random.choice([3, 5, 7])
        angle = random.uniform(0, 180)
        
        # Create motion blur kernel
        kernel = np.zeros((kernel_size, kernel_size))
        kernel[kernel_size // 2, :] = 1
        
        # Rotate kernel
        M = cv2.getRotationMatrix2D((kernel_size // 2, kernel_size // 2), angle, 1)
        kernel = cv2.warpAffine(kernel, M, (kernel_size, kernel_size))
        kernel = kernel / np.sum(kernel)
        
        # Apply blur
        blurred = cv2.filter2D(img_array, -1, kernel)
        
        return Image.fromarray(blurred)
    
    def _vary_stroke_thickness(self, image, variation):
        """Vary stroke thickness using morphological operations"""
        # Convert to grayscale for processing
        gray = image.convert('L')
        img_array = np.array(gray)
        
        # Invert (signatures are usually dark on light)
        img_array = 255 - img_array
        
        # Random morphological operation
        kernel_size = random.choice([2, 3])
        kernel = np.ones((kernel_size, kernel_size), np.uint8)
        
        if random.random() < 0.5:
            # Dilation (thicker strokes)
            processed = cv2.dilate(img_array, kernel, iterations=1)
        else:
            # Erosion (thinner strokes)
            processed = cv2.erode(img_array, kernel, iterations=1)
        
        # Invert back
        processed = 255 - processed
        
        # Convert back to RGB
        result = Image.fromarray(processed).convert('RGB')
        
        # Blend with original
        blend_factor = variation
        result = Image.blend(image, result, blend_factor)
        
        return result
    
    def _add_noise(self, image):
        """Add realistic noise to signature"""
        img_array = np.array(image).astype(np.float32)
        
        noise_type = random.choice(['gaussian', 'salt_pepper', 'speckle'])
        
        if noise_type == 'gaussian':
            # Gaussian noise
            noise = np.random.randn(*img_array.shape) * random.uniform(5, 15)
            img_array = img_array + noise
        
        elif noise_type == 'salt_pepper':
            # Salt and pepper noise
            prob = random.uniform(0.01, 0.03)
            mask = np.random.random(img_array.shape[:2])
            img_array[mask < prob/2] = 0
            img_array[mask > 1 - prob/2] = 255
        
        else:  # speckle
            # Speckle noise
            noise = np.random.randn(*img_array.shape) * 0.1
            img_array = img_array + img_array * noise
        
        # Clip values
        img_array = np.clip(img_array, 0, 255).astype(np.uint8)
        
        return Image.fromarray(img_array)
    
    def _apply_shear(self, image, shear_range):
        """Apply shear transformation"""
        img_array = np.array(image)
        height, width = img_array.shape[:2]
        
        # Random shear values
        shear_x = random.uniform(-shear_range, shear_range)
        shear_y = random.uniform(-shear_range, shear_range)
        
        # Shear matrix
        M = np.array([
            [1, shear_x, -shear_x * height/2],
            [shear_y, 1, -shear_y * width/2]
        ], dtype=np.float32)
        
        # Apply transformation
        sheared = cv2.warpAffine(img_array, M, (width, height), 
                                borderMode=cv2.BORDER_CONSTANT,
                                borderValue=(255, 255, 255))
        
        return Image.fromarray(sheared)
    
    def augment_batch(self, images, labels, augmentation_factor=2):
        """Augment a batch of images with their labels"""
        # CRITICAL FIX: Ensure augmentation_factor is always an integer
        try:
            if augmentation_factor is None:
                augmentation_factor = 2
            augmentation_factor = int(round(augmentation_factor))
            if augmentation_factor < 1:
                augmentation_factor = 1
        except (TypeError, ValueError):
            logger.warning(f"Invalid augmentation_factor: {augmentation_factor}, using default of 2")
            augmentation_factor = 2
        
        augmented_images = []
        augmented_labels = []
        
        for img, label in zip(images, labels):
            # Always include original
            augmented_images.append(img)
            augmented_labels.append(label)
            
            # Generate augmented versions
            # Ensure we're using integer for range
            num_augmentations = max(0, augmentation_factor - 1)
            for _ in range(num_augmentations):
                aug_img = self.augment_single(img, is_genuine=label)
                augmented_images.append(aug_img)
                augmented_labels.append(label)
        
        logger.info(f"Augmented {len(images)} images to {len(augmented_images)} samples (factor: {augmentation_factor})")
        
        return augmented_images, augmented_labels

cpu_optimization.py
import tensorflow as tf
import os
import logging
import numpy as np  # ADD THIS MISSING IMPORT!

logger = logging.getLogger(__name__)


def configure_tensorflow_for_cpu():
    """Configure TensorFlow for optimal CPU performance on Ryzen processors"""
    
    # Set CPU as the only visible device
    try:
        # Hide GPU devices
        tf.config.set_visible_devices([], 'GPU')
        
        # Get CPU devices
        cpus = tf.config.list_physical_devices('CPU')
        if cpus:
            logger.info(f"Found {len(cpus)} CPU devices")
                
    except RuntimeError as e:
        logger.warning(f"Could not configure CPU devices: {e}")
    
    # Set number of inter-op and intra-op threads for Ryzen
    # Ryzen 5 3400G has 4 cores, 8 threads
    num_cores = os.cpu_count() or 4
    optimal_threads = min(num_cores - 1, 6)  # Leave one core free for system
    
    tf.config.threading.set_inter_op_parallelism_threads(optimal_threads)
    tf.config.threading.set_intra_op_parallelism_threads(optimal_threads)
    
    # Enable XLA JIT compilation for faster CPU execution
    tf.config.optimizer.set_jit(True)
    
    # Set memory allocation options
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce logging
    
    # AMD specific optimizations
    os.environ['OMP_NUM_THREADS'] = str(optimal_threads)
    os.environ['MKL_NUM_THREADS'] = str(optimal_threads)
    os.environ['OPENBLAS_NUM_THREADS'] = str(optimal_threads)
    
    logger.info(f"TensorFlow configured for CPU with {optimal_threads} threads")
    return optimal_threads


def get_optimal_batch_size(model_size='medium', ram_gb=8):
    """Get optimal batch size based on model size and available RAM"""
    
    # Base recommendations for Ryzen 5 3400G
    batch_sizes = {
        'small': {'8gb': 32, '16gb': 64, '32gb': 128},
        'medium': {'8gb': 16, '16gb': 32, '32gb': 64},
        'large': {'8gb': 8, '16gb': 16, '32gb': 32}
    }
    
    # Determine RAM category
    if ram_gb <= 8:
        ram_key = '8gb'
    elif ram_gb <= 16:
        ram_key = '16gb'
    else:
        ram_key = '32gb'
    
    return batch_sizes.get(model_size, batch_sizes['medium'])[ram_key]


class CPUDataGenerator(tf.keras.utils.Sequence):
    """Optimized data generator for CPU training"""
    
    def __init__(self, x_a, x_b, y, batch_size=16, shuffle=True):
        self.x_a = x_a
        self.x_b = x_b
        self.y = y
        self.batch_size = int(batch_size)  # Ensure batch_size is integer
        self.shuffle = shuffle
        self.indices = np.arange(len(self.y))
        if self.shuffle:
            np.random.shuffle(self.indices)
    
    def __len__(self):
        return (len(self.y) + self.batch_size - 1) // self.batch_size
    
    def __getitem__(self, index):
        start = index * self.batch_size
        end = min((index + 1) * self.batch_size, len(self.y))
        batch_indices = self.indices[start:end]
        
        batch_x_a = self.x_a[batch_indices]
        batch_x_b = self.x_b[batch_indices]
        batch_y = self.y[batch_indices]
        
        return [batch_x_a, batch_x_b], batch_y
    
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)


def create_cpu_optimized_model_checkpoint():
    """Create model checkpoint callback optimized for CPU"""
    return tf.keras.callbacks.ModelCheckpoint(
        filepath='checkpoints/model_{epoch:02d}_{val_auc:.3f}.keras',
        monitor='val_auc',
        mode='max',
        save_best_only=True,
        save_weights_only=False,
        verbose=1,
        save_freq='epoch',
        options=tf.saved_model.SaveOptions(
            experimental_io_device='/job:localhost'  # Save on CPU
        ) if hasattr(tf.saved_model, 'SaveOptions') else None
    )


def profile_model_performance(model, test_data):
    """Profile model performance on CPU"""
    import time
    
    # Warmup
    _ = model.predict(test_data[:2], verbose=0)
    
    # Measure inference time
    num_samples = len(test_data)
    start_time = time.time()
    predictions = model.predict(test_data, verbose=0, batch_size=1)
    end_time = time.time()
    
    total_time = end_time - start_time
    avg_time = total_time / num_samples * 1000  # ms per sample
    
    return {
        'total_inference_time': total_time,
        'avg_inference_time_ms': avg_time,
        'samples_per_second': num_samples / total_time
    }
image_processing.py
from PIL import Image
import numpy as np
import cv2
import io
import logging

logger = logging.getLogger(__name__)


def preprocess_image(image, target_size, enhance=True):
    """Enhanced preprocessing for signature images"""
    try:
        # Handle different input types
        if isinstance(image, bytes):
            image = Image.open(io.BytesIO(image))
        elif isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError(f"Unsupported image type: {type(image)}")
        
        # Handle PNG specifically (CEDAR dataset compatibility)
        if image.format == 'PNG' or hasattr(image, 'mode'):
            if image.mode == 'RGBA':
                # Convert RGBA to RGB with white background
                background = Image.new('RGB', image.size, (255, 255, 255))
                background.paste(image, mask=image.split()[3] if len(image.split()) > 3 else None)
                image = background
            elif image.mode == 'LA' or image.mode == 'L':
                # Convert grayscale to RGB
                image = image.convert('RGB')
            elif image.mode == 'P':
                # Convert palette images to RGB
                image = image.convert('RGB')
            elif image.mode != 'RGB':
                # Fallback conversion
                image = image.convert('RGB')
        
        # Optional enhancement for better feature extraction
        if enhance:
            image = enhance_signature_image(image)
        
        # Smart resize maintaining aspect ratio
        image = smart_resize(image, target_size)
        
        return image
    
    except Exception as e:
        logger.error(f"Image preprocessing failed: {e}")
        raise


def smart_resize(image, target_size):
    """Resize image maintaining aspect ratio with padding"""
    # Get current size
    width, height = image.size
    
    # Calculate scaling factor
    scale = min(target_size / width, target_size / height)
    
    # Calculate new size
    new_width = int(width * scale)
    new_height = int(height * scale)
    
    # Resize image
    image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
    
    # Create padded image
    padded = Image.new('RGB', (target_size, target_size), (255, 255, 255))
    
    # Calculate padding
    left = (target_size - new_width) // 2
    top = (target_size - new_height) // 2
    
    # Paste resized image
    padded.paste(image, (left, top))
    
    return padded


def enhance_signature_image(image):
    """Enhance signature image for better feature extraction"""
    try:
        # Convert to numpy array
        img_array = np.array(image)
        
        # Convert to grayscale for processing
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        
        # Apply adaptive histogram equalization for better contrast
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)
        
        # Denoise
        denoised = cv2.fastNlMeansDenoising(enhanced, h=10)
        
        # Apply slight Gaussian blur to smooth edges
        smoothed = cv2.GaussianBlur(denoised, (3, 3), 0.5)
        
        # Convert back to RGB
        enhanced_rgb = cv2.cvtColor(smoothed, cv2.COLOR_GRAY2RGB)
        
        # Enhance contrast slightly
        alpha = 1.1  # Contrast control
        beta = -10   # Brightness control
        enhanced_rgb = cv2.convertScaleAbs(enhanced_rgb, alpha=alpha, beta=beta)
        
        return Image.fromarray(enhanced_rgb)
    
    except Exception as e:
        logger.warning(f"Image enhancement failed, using original: {e}")
        return image


def get_image_quality_score(image):
    """Calculate comprehensive image quality score"""
    try:
        # Convert to grayscale for analysis
        if isinstance(image, Image.Image):
            gray = np.array(image.convert('L'))
        else:
            gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)
        
        # Multiple quality metrics
        metrics = {}
        
        # 1. Variance (sharpness/detail)
        metrics['variance'] = np.var(gray)
        
        # 2. Laplacian variance (focus measure)
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        metrics['focus'] = laplacian.var()
        
        # 3. Edge density (signature structure)
        edges = cv2.Canny(gray, 50, 150)
        metrics['edge_density'] = np.sum(edges > 0) / edges.size
        
        # 4. Contrast (dynamic range)
        metrics['contrast'] = gray.std()
        
        # 5. Entropy (information content)
        hist, _ = np.histogram(gray, bins=256, range=(0, 256))
        hist = hist / hist.sum()
        hist = hist[hist > 0]  # Remove zeros for log
        metrics['entropy'] = -np.sum(hist * np.log2(hist))
        
        # Normalize and combine metrics
        weights = {
            'variance': 0.2,
            'focus': 0.3,
            'edge_density': 0.2,
            'contrast': 0.15,
            'entropy': 0.15
        }
        
        # Normalize each metric to 0-1 range
        normalized_scores = {}
        normalized_scores['variance'] = min(metrics['variance'] / 5000, 1.0)
        normalized_scores['focus'] = min(metrics['focus'] / 1000, 1.0)
        normalized_scores['edge_density'] = min(metrics['edge_density'] * 10, 1.0)
        normalized_scores['contrast'] = min(metrics['contrast'] / 100, 1.0)
        normalized_scores['entropy'] = metrics['entropy'] / 8.0  # Max entropy is 8 for 256 bins
        
        # Weighted average
        quality_score = sum(normalized_scores[key] * weights[key] for key in weights)
        
        return float(min(quality_score, 1.0))
    
    except Exception as e:
        logger.error(f"Quality score calculation failed: {e}")
        return 0.5  # Default middle score on error


def validate_signature_image(image, min_quality_score=0.3):
    """Validate if image is suitable for signature verification"""
    try:
        # Get quality score
        quality = get_image_quality_score(image)
        
        # Check minimum quality
        if quality < min_quality_score:
            return False, f"Image quality too low: {quality:.2f}"
        
        # Check image dimensions
        width, height = image.size if hasattr(image, 'size') else image.shape[:2]
        if width < 50 or height < 50:
            return False, "Image too small (minimum 50x50)"
        
        if width > 4000 or height > 4000:
            return False, "Image too large (maximum 4000x4000)"
        
        # Check if mostly blank
        gray = np.array(image.convert('L') if hasattr(image, 'convert') else image)
        non_white_ratio = np.sum(gray < 250) / gray.size
        if non_white_ratio < 0.01:
            return False, "Image appears to be blank"
        
        return True, "Valid"
    
    except Exception as e:
        return False, f"Validation error: {str(e)}"


main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import os
from config import settings
from api.training import router as training_router
from api.verification import router as verification_router
from api.progress import router as progress_router
from api.versioning import router as versioning_router

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    os.makedirs(settings.LOCAL_MODELS_DIR, exist_ok=True)
    print(f"🚀 AI Training Backend started on {settings.HOST}:{settings.PORT}")
    print(f"📁 Local models directory: {settings.LOCAL_MODELS_DIR}")
    yield
    # Shutdown
    print("🛑 AI Training Backend stopped")

app = FastAPI(
    title="Signature AI Training API",
    description="AI-powered signature verification training backend",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure this properly for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(training_router, prefix="/api/training", tags=["training"])
app.include_router(verification_router, prefix="/api/verification", tags=["verification"])
app.include_router(progress_router, prefix="/api/progress", tags=["progress"])
app.include_router(versioning_router, prefix="/api/versioning", tags=["versioning"])

@app.get("/")
async def root():
    return {
        "message": "Signature AI Training API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG
    )


config.py
import os
from dotenv import load_dotenv

load_dotenv()

class Settings:
    # Supabase Configuration
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")
    
    # Server Configuration
    HOST = os.getenv("HOST", "0.0.0.0")
    PORT = int(os.getenv("PORT", 8000))
    DEBUG = os.getenv("DEBUG", "True").lower() == "true"
    
    # Model Configuration
    MODEL_IMAGE_SIZE = int(os.getenv("MODEL_IMAGE_SIZE", 224))
    MODEL_BATCH_SIZE = int(os.getenv("MODEL_BATCH_SIZE", 32))
    MODEL_EPOCHS = int(os.getenv("MODEL_EPOCHS", 50))
    MODEL_LEARNING_RATE = float(os.getenv("MODEL_LEARNING_RATE", 0.001))

        # CPU Optimization
    USE_CPU_OPTIMIZATION: bool = True
    CPU_THREADS: int = 6  # For Ryzen 5 3400G
    
    # Storage Configuration
    LOCAL_MODELS_DIR = os.getenv("LOCAL_MODELS_DIR", "./models")
    SUPABASE_BUCKET = os.getenv("SUPABASE_BUCKET", "models")
    
    # Training Configuration
    MIN_GENUINE_SAMPLES = int(os.getenv("MIN_GENUINE_SAMPLES", 10))
    MIN_FORGED_SAMPLES = int(os.getenv("MIN_FORGED_SAMPLES", 5))
    MAX_TRAINING_TIME = int(os.getenv("MAX_TRAINING_TIME", 3600))

    # Verification Settings
    DEFAULT_SIMILARITY_THRESHOLD: float = 0.7
    USE_ADAPTIVE_THRESHOLD: bool = True
    
        # Anti-Spoofing
    ENABLE_ANTISPOOFING: bool = True
    SPOOFING_THRESHOLD: float = 0.6
    
    # Model Versioning
    ENABLE_MODEL_VERSIONING: bool = True
    MAX_MODEL_VERSIONS: int = 5
    
    # Performance Monitoring
    ENABLE_PERFORMANCE_MONITORING: bool = True
    LOG_LEVEL: str = "INFO"
    
    class Config:
        env_file = ".env"

settings = Settings()

# Initialize CPU optimization on import
if settings.USE_CPU_OPTIMIZATION:
    from utils.cpu_optimization import configure_tensorflow_for_cpu
    configure_tensorflow_for_cpu()

